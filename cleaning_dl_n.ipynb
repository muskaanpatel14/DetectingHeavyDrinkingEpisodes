{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import scipy\n",
    "from scipy.stats import skew, kurtosis\n",
    "import tensorflow as tf\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pid</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>JB3156</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CC6740</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SA0297</td>\n",
       "      <td>0.0758</td>\n",
       "      <td>0.0273</td>\n",
       "      <td>-0.0102</td>\n",
       "      <td>1493733882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SA0297</td>\n",
       "      <td>-0.0359</td>\n",
       "      <td>0.0794</td>\n",
       "      <td>0.0037</td>\n",
       "      <td>1493733882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SA0297</td>\n",
       "      <td>-0.2427</td>\n",
       "      <td>-0.0861</td>\n",
       "      <td>-0.0163</td>\n",
       "      <td>1493733882</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      pid       x       y       z        time\n",
       "0  JB3156  0.0000  0.0000  0.0000           0\n",
       "1  CC6740  0.0000  0.0000  0.0000           0\n",
       "2  SA0297  0.0758  0.0273 -0.0102  1493733882\n",
       "3  SA0297 -0.0359  0.0794  0.0037  1493733882\n",
       "4  SA0297 -0.2427 -0.0861 -0.0163  1493733882"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read Accelerometer Data\n",
    "acc_data = pd.read_csv('../data/all_accelerometer_data_pids_13.csv')\n",
    "\n",
    "\n",
    "def get_time_value(x):\n",
    "  # x is ms. it is divided by 1000 to get microsecond\n",
    "  t = datetime.datetime.fromtimestamp(x/1000.0)\n",
    "  t = t.replace(microsecond = 0)\n",
    "  return int(t.timestamp())\n",
    "\n",
    "acc_data['window10'] = acc_data['time'].apply(get_time_value)\n",
    "acc_data = acc_data.drop(columns=\"time\")\n",
    "acc_data = acc_data.rename(columns = {\"window10\": \"time\"})\n",
    "\n",
    "acc_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['JB3156', 'CC6740', 'SA0297', 'PC6771', 'BK7610', 'DC6359',\n",
       "       'MC7070', 'MJ8002', 'BU4707', 'JR8022', 'HV0618', 'SF3079',\n",
       "       'DK3500'], dtype=object)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_data['pid'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>TAC_Reading</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5.700000e+01</td>\n",
       "      <td>57.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.493758e+09</td>\n",
       "      <td>0.228070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.841595e+04</td>\n",
       "      <td>0.423318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.493719e+09</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.493729e+09</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.493756e+09</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.493782e+09</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.493808e+09</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          timestamp  TAC_Reading\n",
       "count  5.700000e+01    57.000000\n",
       "mean   1.493758e+09     0.228070\n",
       "std    2.841595e+04     0.423318\n",
       "min    1.493719e+09     0.000000\n",
       "25%    1.493729e+09     0.000000\n",
       "50%    1.493756e+09     0.000000\n",
       "75%    1.493782e+09     0.000000\n",
       "max    1.493808e+09     1.000000"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read clean tac data for pid = BK7610\n",
    "clean_tac_data = pd.read_csv('../data/clean_tac/BK7610_clean_TAC.csv')\n",
    "# clean_tac_data = pd.read_csv('../data/clean_tac/JB3156_clean_TAC.csv')\n",
    "clean_tac_data[\"tac\"] = np.where(clean_tac_data[\"TAC_Reading\"] > 0.08, 1, 0)\n",
    "clean_tac_data = clean_tac_data.drop(columns=\"TAC_Reading\")\n",
    "clean_tac_data = clean_tac_data.rename(columns={\"tac\": \"TAC_Reading\"})\n",
    "clean_tac_data.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['BK7610'], dtype=object)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filtering for specific PID (temps)\n",
    "acc_data_pid = acc_data[acc_data.pid == \"BK7610\"]\n",
    "acc_data_pid['pid'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.225727e+06</td>\n",
       "      <td>1.225727e+06</td>\n",
       "      <td>1.225727e+06</td>\n",
       "      <td>1.225727e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-6.497703e-03</td>\n",
       "      <td>7.507374e-03</td>\n",
       "      <td>2.747567e-03</td>\n",
       "      <td>1.493752e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.380473e-01</td>\n",
       "      <td>1.387602e-01</td>\n",
       "      <td>1.279124e-01</td>\n",
       "      <td>9.276766e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-4.274800e+00</td>\n",
       "      <td>-6.948900e+00</td>\n",
       "      <td>-5.277200e+00</td>\n",
       "      <td>1.493736e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-9.400000e-03</td>\n",
       "      <td>-6.000000e-03</td>\n",
       "      <td>-7.300000e-03</td>\n",
       "      <td>1.493744e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000e-04</td>\n",
       "      <td>1.000000e-04</td>\n",
       "      <td>5.700000e-03</td>\n",
       "      <td>1.493752e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8.300000e-03</td>\n",
       "      <td>9.400000e-03</td>\n",
       "      <td>1.140000e-02</td>\n",
       "      <td>1.493760e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>6.450300e+00</td>\n",
       "      <td>5.344100e+00</td>\n",
       "      <td>4.656500e+00</td>\n",
       "      <td>1.493768e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  x             y             z          time\n",
       "count  1.225727e+06  1.225727e+06  1.225727e+06  1.225727e+06\n",
       "mean  -6.497703e-03  7.507374e-03  2.747567e-03  1.493752e+09\n",
       "std    1.380473e-01  1.387602e-01  1.279124e-01  9.276766e+03\n",
       "min   -4.274800e+00 -6.948900e+00 -5.277200e+00  1.493736e+09\n",
       "25%   -9.400000e-03 -6.000000e-03 -7.300000e-03  1.493744e+09\n",
       "50%    1.000000e-04  1.000000e-04  5.700000e-03  1.493752e+09\n",
       "75%    8.300000e-03  9.400000e-03  1.140000e-02  1.493760e+09\n",
       "max    6.450300e+00  5.344100e+00  4.656500e+00  1.493768e+09"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_data_pid.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1225727, 5)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_data_pid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1493735870],\n",
       " [0, 1493735870],\n",
       " [0, 1493735870],\n",
       " [0, 1493735870],\n",
       " [0, 1493735870],\n",
       " [0, 1493735870],\n",
       " [0, 1493735870],\n",
       " [0, 1493735870],\n",
       " [0, 1493735870],\n",
       " [0, 1493735870],\n",
       " [0, 1493735870],\n",
       " [0, 1493735870],\n",
       " [0, 1493735870],\n",
       " [0, 1493735870],\n",
       " [0, 1493735871],\n",
       " [0, 1493735871],\n",
       " [0, 1493735871],\n",
       " [0, 1493735871],\n",
       " [0, 1493735871],\n",
       " [0, 1493735871],\n",
       " [0, 1493735871],\n",
       " [0, 1493735871],\n",
       " [0, 1493735871],\n",
       " [0, 1493735871],\n",
       " [0, 1493735871],\n",
       " [0, 1493735871],\n",
       " [0, 1493735871],\n",
       " [0, 1493735871],\n",
       " [0, 1493735871],\n",
       " [0, 1493735871],\n",
       " [0, 1493735871],\n",
       " [0, 1493735871],\n",
       " [0, 1493735871],\n",
       " [0, 1493735871],\n",
       " [0, 1493735871],\n",
       " [0, 1493735871],\n",
       " [0, 1493735871],\n",
       " [0, 1493735871],\n",
       " [0, 1493735871],\n",
       " [0, 1493735871],\n",
       " [0, 1493735871],\n",
       " [0, 1493735871],\n",
       " [0, 1493735871],\n",
       " [0, 1493735871],\n",
       " [0, 1493735871],\n",
       " [0, 1493735871],\n",
       " [0, 1493735871],\n",
       " [0, 1493735871],\n",
       " [0, 1493735871],\n",
       " [0, 1493735871],\n",
       " [0, 1493735871],\n",
       " [0, 1493735871],\n",
       " [0, 1493735871],\n",
       " [0, 1493735871],\n",
       " [0, 1493735872],\n",
       " [0, 1493735872],\n",
       " [0, 1493735872],\n",
       " [0, 1493735872],\n",
       " [0, 1493735872],\n",
       " [0, 1493735872],\n",
       " [0, 1493735872],\n",
       " [0, 1493735872],\n",
       " [0, 1493735872],\n",
       " [0, 1493735872],\n",
       " [0, 1493735872],\n",
       " [0, 1493735872],\n",
       " [0, 1493735872],\n",
       " [0, 1493735872],\n",
       " [0, 1493735872],\n",
       " [0, 1493735872],\n",
       " [0, 1493735872],\n",
       " [0, 1493735872],\n",
       " [0, 1493735872],\n",
       " [0, 1493735872],\n",
       " [0, 1493735872],\n",
       " [0, 1493735872],\n",
       " [0, 1493735872],\n",
       " [0, 1493735872],\n",
       " [0, 1493735872],\n",
       " [0, 1493735872],\n",
       " [0, 1493735872],\n",
       " [0, 1493735872],\n",
       " [0, 1493735872],\n",
       " [0, 1493735872],\n",
       " [0, 1493735872],\n",
       " [0, 1493735872],\n",
       " [0, 1493735872],\n",
       " [0, 1493735872],\n",
       " [0, 1493735872],\n",
       " [0, 1493735872],\n",
       " [0, 1493735872],\n",
       " [0, 1493735872],\n",
       " [0, 1493735872],\n",
       " [0, 1493735872],\n",
       " [0, 1493735873],\n",
       " [0, 1493735873],\n",
       " [0, 1493735873],\n",
       " [0, 1493735873],\n",
       " [0, 1493735873],\n",
       " [0, 1493735873],\n",
       " [0, 1493735873],\n",
       " [0, 1493735873],\n",
       " [0, 1493735873],\n",
       " [0, 1493735873],\n",
       " [0, 1493735873],\n",
       " [0, 1493735873],\n",
       " [0, 1493735873],\n",
       " [0, 1493735873],\n",
       " [0, 1493735873],\n",
       " [0, 1493735873],\n",
       " [0, 1493735873],\n",
       " [0, 1493735873],\n",
       " [0, 1493735873],\n",
       " [0, 1493735873],\n",
       " [0, 1493735873],\n",
       " [0, 1493735873],\n",
       " [0, 1493735873],\n",
       " [0, 1493735873],\n",
       " [0, 1493735873],\n",
       " [0, 1493735873],\n",
       " [0, 1493735873],\n",
       " [0, 1493735873],\n",
       " [0, 1493735873],\n",
       " [0, 1493735873],\n",
       " [0, 1493735873],\n",
       " [0, 1493735873],\n",
       " [0, 1493735873],\n",
       " [0, 1493735873],\n",
       " [0, 1493735873],\n",
       " [0, 1493735873],\n",
       " [0, 1493735873],\n",
       " [0, 1493735873],\n",
       " [0, 1493735873],\n",
       " [0, 1493735873],\n",
       " [0, 1493735874],\n",
       " [0, 1493735874],\n",
       " [0, 1493735874],\n",
       " [0, 1493735874],\n",
       " [0, 1493735874],\n",
       " [0, 1493735874],\n",
       " [0, 1493735874],\n",
       " [0, 1493735874],\n",
       " [0, 1493735874],\n",
       " [0, 1493735874],\n",
       " [0, 1493735874],\n",
       " [0, 1493735874],\n",
       " [0, 1493735874],\n",
       " [0, 1493735874],\n",
       " [0, 1493735874],\n",
       " [0, 1493735874],\n",
       " [0, 1493735874],\n",
       " [0, 1493735874],\n",
       " [0, 1493735874],\n",
       " [0, 1493735874],\n",
       " [0, 1493735874],\n",
       " [0, 1493735874],\n",
       " [0, 1493735874],\n",
       " [0, 1493735874],\n",
       " [0, 1493735874],\n",
       " [0, 1493735874],\n",
       " [0, 1493735874],\n",
       " [0, 1493735874],\n",
       " [0, 1493735874],\n",
       " [0, 1493735874],\n",
       " [0, 1493735874],\n",
       " [0, 1493735874],\n",
       " [0, 1493735874],\n",
       " [0, 1493735874],\n",
       " [0, 1493735874],\n",
       " [0, 1493735874],\n",
       " [0, 1493735874],\n",
       " [0, 1493735874],\n",
       " [0, 1493735874],\n",
       " [0, 1493735874],\n",
       " [0, 1493735875],\n",
       " [0, 1493735875],\n",
       " [0, 1493735875],\n",
       " [0, 1493735875],\n",
       " [0, 1493735875],\n",
       " [0, 1493735875],\n",
       " [0, 1493735875],\n",
       " [0, 1493735875],\n",
       " [0, 1493735875],\n",
       " [0, 1493735875],\n",
       " [0, 1493735875],\n",
       " [0, 1493735875],\n",
       " [0, 1493735875],\n",
       " [0, 1493735875],\n",
       " [0, 1493735875],\n",
       " [0, 1493735875],\n",
       " [0, 1493735875],\n",
       " [0, 1493735875],\n",
       " [0, 1493735875],\n",
       " [0, 1493735875],\n",
       " [0, 1493735875],\n",
       " [0, 1493735875],\n",
       " [0, 1493735875],\n",
       " [0, 1493735875],\n",
       " [0, 1493735875],\n",
       " [0, 1493735875],\n",
       " [0, 1493735875],\n",
       " [0, 1493735875],\n",
       " [0, 1493735875],\n",
       " [0, 1493735875],\n",
       " [0, 1493735875],\n",
       " [0, 1493735875],\n",
       " [0, 1493735875],\n",
       " [0, 1493735875],\n",
       " [0, 1493735875],\n",
       " [0, 1493735875],\n",
       " [0, 1493735875],\n",
       " [0, 1493735875],\n",
       " [0, 1493735875],\n",
       " [0, 1493735875],\n",
       " [0, 1493735876],\n",
       " [0, 1493735876],\n",
       " [0, 1493735876],\n",
       " [0, 1493735876],\n",
       " [0, 1493735876],\n",
       " [0, 1493735876],\n",
       " [0, 1493735876],\n",
       " [0, 1493735876],\n",
       " [0, 1493735876],\n",
       " [0, 1493735876],\n",
       " [0, 1493735876],\n",
       " [0, 1493735876],\n",
       " [0, 1493735876],\n",
       " [0, 1493735876],\n",
       " [0, 1493735876],\n",
       " [0, 1493735876],\n",
       " [0, 1493735876],\n",
       " [0, 1493735876],\n",
       " [0, 1493735876],\n",
       " [0, 1493735876],\n",
       " [0, 1493735876],\n",
       " [0, 1493735876],\n",
       " [0, 1493735876],\n",
       " [0, 1493735876],\n",
       " [0, 1493735876],\n",
       " [0, 1493735876],\n",
       " [0, 1493735876],\n",
       " [0, 1493735876],\n",
       " [0, 1493735876],\n",
       " [0, 1493735876],\n",
       " [0, 1493735876],\n",
       " [0, 1493735876],\n",
       " [0, 1493735876],\n",
       " [0, 1493735876],\n",
       " [0, 1493735876],\n",
       " [0, 1493735876],\n",
       " [0, 1493735876],\n",
       " [0, 1493735876],\n",
       " [0, 1493735876],\n",
       " [0, 1493735876],\n",
       " [0, 1493735877],\n",
       " [0, 1493735877],\n",
       " [0, 1493735877],\n",
       " [0, 1493735877],\n",
       " [0, 1493735877],\n",
       " [0, 1493735877],\n",
       " [0, 1493735877],\n",
       " [0, 1493735877],\n",
       " [0, 1493735877],\n",
       " [0, 1493735877],\n",
       " [0, 1493735877],\n",
       " [0, 1493735877],\n",
       " [0, 1493735877],\n",
       " [0, 1493735877],\n",
       " [0, 1493735877],\n",
       " [0, 1493735877],\n",
       " [0, 1493735877],\n",
       " [0, 1493735877],\n",
       " [0, 1493735877],\n",
       " [0, 1493735877],\n",
       " [0, 1493735877],\n",
       " [0, 1493735877],\n",
       " [0, 1493735877],\n",
       " [0, 1493735877],\n",
       " [0, 1493735877],\n",
       " [0, 1493735877],\n",
       " [0, 1493735877],\n",
       " [0, 1493735877],\n",
       " [0, 1493735877],\n",
       " [0, 1493735877],\n",
       " [0, 1493735877],\n",
       " [0, 1493735877],\n",
       " [0, 1493735877],\n",
       " [0, 1493735877],\n",
       " [0, 1493735877],\n",
       " [0, 1493735877],\n",
       " [0, 1493735877],\n",
       " [0, 1493735877],\n",
       " [0, 1493735877],\n",
       " [0, 1493735877],\n",
       " [0, 1493735878],\n",
       " [0, 1493735878],\n",
       " [0, 1493735878],\n",
       " [0, 1493735878],\n",
       " [0, 1493735878],\n",
       " [0, 1493735878],\n",
       " [0, 1493735878],\n",
       " [0, 1493735878],\n",
       " [0, 1493735878],\n",
       " [0, 1493735878],\n",
       " [0, 1493735878],\n",
       " [0, 1493735878],\n",
       " [0, 1493735878],\n",
       " [0, 1493735878],\n",
       " [0, 1493735878],\n",
       " [0, 1493735878],\n",
       " [0, 1493735878],\n",
       " [0, 1493735878],\n",
       " [0, 1493735878],\n",
       " [0, 1493735878],\n",
       " [0, 1493735878],\n",
       " [0, 1493735878],\n",
       " [0, 1493735878],\n",
       " [0, 1493735878],\n",
       " [0, 1493735878],\n",
       " [0, 1493735878],\n",
       " [0, 1493735878],\n",
       " [0, 1493735878],\n",
       " [0, 1493735878],\n",
       " [0, 1493735878],\n",
       " [0, 1493735878],\n",
       " [0, 1493735878],\n",
       " [0, 1493735878],\n",
       " [0, 1493735878],\n",
       " [0, 1493735878],\n",
       " [0, 1493735878],\n",
       " [0, 1493735878],\n",
       " [0, 1493735878],\n",
       " [0, 1493735878],\n",
       " [0, 1493735878],\n",
       " [0, 1493735879],\n",
       " [0, 1493735879],\n",
       " [0, 1493735879],\n",
       " [0, 1493735879],\n",
       " [0, 1493735879],\n",
       " [0, 1493735879],\n",
       " [0, 1493735879],\n",
       " [0, 1493735879],\n",
       " [0, 1493735879],\n",
       " [0, 1493735879],\n",
       " [0, 1493735879],\n",
       " [0, 1493735879],\n",
       " [0, 1493735879],\n",
       " [0, 1493735879],\n",
       " [0, 1493735879],\n",
       " [0, 1493735879],\n",
       " [0, 1493735879],\n",
       " [0, 1493735879],\n",
       " [0, 1493735879],\n",
       " [0, 1493735879],\n",
       " [0, 1493735879],\n",
       " [0, 1493735879],\n",
       " [0, 1493735879],\n",
       " [0, 1493735879],\n",
       " [0, 1493735879],\n",
       " [0, 1493735879],\n",
       " [0, 1493735879],\n",
       " [0, 1493735879],\n",
       " [0, 1493735879],\n",
       " [0, 1493735879],\n",
       " [0, 1493735879],\n",
       " [0, 1493735879],\n",
       " [0, 1493735879],\n",
       " [0, 1493735879],\n",
       " [0, 1493735879],\n",
       " [0, 1493735879],\n",
       " [0, 1493735879],\n",
       " [0, 1493735879],\n",
       " [0, 1493735879],\n",
       " [0, 1493735879],\n",
       " [0, 1493735880],\n",
       " [0, 1493735880],\n",
       " [0, 1493735880],\n",
       " [0, 1493735880],\n",
       " [0, 1493735880],\n",
       " [0, 1493735880],\n",
       " [0, 1493735880],\n",
       " [0, 1493735880],\n",
       " [0, 1493735880],\n",
       " [0, 1493735880],\n",
       " [0, 1493735880],\n",
       " [0, 1493735880],\n",
       " [0, 1493735880],\n",
       " [0, 1493735880],\n",
       " [0, 1493735880],\n",
       " [0, 1493735880],\n",
       " [0, 1493735880],\n",
       " [0, 1493735880],\n",
       " [0, 1493735880],\n",
       " [0, 1493735880],\n",
       " [0, 1493735880],\n",
       " [0, 1493735880],\n",
       " [0, 1493735880],\n",
       " [0, 1493735880],\n",
       " [0, 1493735880],\n",
       " [0, 1493735880],\n",
       " [0, 1493735880],\n",
       " [0, 1493735880],\n",
       " [0, 1493735880],\n",
       " [0, 1493735880],\n",
       " [0, 1493735880],\n",
       " [0, 1493735880],\n",
       " [0, 1493735880],\n",
       " [0, 1493735880],\n",
       " [0, 1493735880],\n",
       " [0, 1493735880],\n",
       " [0, 1493735880],\n",
       " [0, 1493735880],\n",
       " [0, 1493735880],\n",
       " [0, 1493735880],\n",
       " [0, 1493735881],\n",
       " [0, 1493735881],\n",
       " [0, 1493735881],\n",
       " [0, 1493735881],\n",
       " [0, 1493735881],\n",
       " [0, 1493735881],\n",
       " [0, 1493735881],\n",
       " [0, 1493735881],\n",
       " [0, 1493735881],\n",
       " [0, 1493735881],\n",
       " [0, 1493735881],\n",
       " [0, 1493735881],\n",
       " [0, 1493735881],\n",
       " [0, 1493735881],\n",
       " [0, 1493735881],\n",
       " [0, 1493735881],\n",
       " [0, 1493735881],\n",
       " [0, 1493735881],\n",
       " [0, 1493735881],\n",
       " [0, 1493735881],\n",
       " [0, 1493735881],\n",
       " [0, 1493735881],\n",
       " [0, 1493735881],\n",
       " [0, 1493735881],\n",
       " [0, 1493735881],\n",
       " [0, 1493735881],\n",
       " [0, 1493735881],\n",
       " [0, 1493735881],\n",
       " [0, 1493735881],\n",
       " [0, 1493735881],\n",
       " [0, 1493735881],\n",
       " [0, 1493735881],\n",
       " [0, 1493735881],\n",
       " [0, 1493735881],\n",
       " [0, 1493735881],\n",
       " [0, 1493735881],\n",
       " [0, 1493735881],\n",
       " [0, 1493735881],\n",
       " [0, 1493735881],\n",
       " [0, 1493735881],\n",
       " [0, 1493735882],\n",
       " [0, 1493735882],\n",
       " [0, 1493735882],\n",
       " [0, 1493735882],\n",
       " [0, 1493735882],\n",
       " [0, 1493735882],\n",
       " [0, 1493735882],\n",
       " [0, 1493735882],\n",
       " [0, 1493735882],\n",
       " [0, 1493735882],\n",
       " [0, 1493735882],\n",
       " [0, 1493735882],\n",
       " [0, 1493735882],\n",
       " [0, 1493735882],\n",
       " [0, 1493735882],\n",
       " [0, 1493735882],\n",
       " [0, 1493735882],\n",
       " [0, 1493735882],\n",
       " [0, 1493735882],\n",
       " [0, 1493735882],\n",
       " [0, 1493735882],\n",
       " [0, 1493735882],\n",
       " [0, 1493735882],\n",
       " [0, 1493735882],\n",
       " [0, 1493735882],\n",
       " [0, 1493735882],\n",
       " [0, 1493735882],\n",
       " [0, 1493735882],\n",
       " [0, 1493735882],\n",
       " [0, 1493735882],\n",
       " [0, 1493735882],\n",
       " [0, 1493735882],\n",
       " [0, 1493735882],\n",
       " [0, 1493735882],\n",
       " [0, 1493735882],\n",
       " [0, 1493735882],\n",
       " [0, 1493735882],\n",
       " [0, 1493735882],\n",
       " [0, 1493735882],\n",
       " [0, 1493735882],\n",
       " [0, 1493735883],\n",
       " [0, 1493735883],\n",
       " [0, 1493735883],\n",
       " [0, 1493735883],\n",
       " [0, 1493735883],\n",
       " [0, 1493735883],\n",
       " [0, 1493735883],\n",
       " [0, 1493735883],\n",
       " [0, 1493735883],\n",
       " [0, 1493735883],\n",
       " [0, 1493735883],\n",
       " [0, 1493735883],\n",
       " [0, 1493735883],\n",
       " [0, 1493735883],\n",
       " [0, 1493735883],\n",
       " [0, 1493735883],\n",
       " [0, 1493735883],\n",
       " [0, 1493735883],\n",
       " [0, 1493735883],\n",
       " [0, 1493735883],\n",
       " [0, 1493735883],\n",
       " [0, 1493735883],\n",
       " [0, 1493735883],\n",
       " [0, 1493735883],\n",
       " [0, 1493735883],\n",
       " [0, 1493735883],\n",
       " [0, 1493735883],\n",
       " [0, 1493735883],\n",
       " [0, 1493735883],\n",
       " [0, 1493735883],\n",
       " [0, 1493735883],\n",
       " [0, 1493735883],\n",
       " [0, 1493735883],\n",
       " [0, 1493735883],\n",
       " [0, 1493735883],\n",
       " [0, 1493735883],\n",
       " [0, 1493735883],\n",
       " [0, 1493735883],\n",
       " [0, 1493735883],\n",
       " [0, 1493735883],\n",
       " [0, 1493735884],\n",
       " [0, 1493735884],\n",
       " [0, 1493735884],\n",
       " [0, 1493735884],\n",
       " [0, 1493735884],\n",
       " [0, 1493735884],\n",
       " [0, 1493735884],\n",
       " [0, 1493735884],\n",
       " [0, 1493735884],\n",
       " [0, 1493735884],\n",
       " [0, 1493735884],\n",
       " [0, 1493735884],\n",
       " [0, 1493735884],\n",
       " [0, 1493735884],\n",
       " [0, 1493735884],\n",
       " [0, 1493735884],\n",
       " [0, 1493735884],\n",
       " [0, 1493735884],\n",
       " [0, 1493735884],\n",
       " [0, 1493735884],\n",
       " [0, 1493735884],\n",
       " [0, 1493735884],\n",
       " [0, 1493735884],\n",
       " [0, 1493735884],\n",
       " [0, 1493735884],\n",
       " [0, 1493735884],\n",
       " [0, 1493735884],\n",
       " [0, 1493735884],\n",
       " [0, 1493735884],\n",
       " [0, 1493735884],\n",
       " [0, 1493735884],\n",
       " [0, 1493735884],\n",
       " [0, 1493735884],\n",
       " [0, 1493735884],\n",
       " [0, 1493735884],\n",
       " [0, 1493735884],\n",
       " [0, 1493735884],\n",
       " [0, 1493735884],\n",
       " [0, 1493735884],\n",
       " [0, 1493735884],\n",
       " [0, 1493735885],\n",
       " [0, 1493735885],\n",
       " [0, 1493735885],\n",
       " [0, 1493735885],\n",
       " [0, 1493735885],\n",
       " [0, 1493735885],\n",
       " [0, 1493735885],\n",
       " [0, 1493735885],\n",
       " [0, 1493735885],\n",
       " [0, 1493735885],\n",
       " [0, 1493735885],\n",
       " [0, 1493735885],\n",
       " [0, 1493735885],\n",
       " [0, 1493735885],\n",
       " [0, 1493735885],\n",
       " [0, 1493735885],\n",
       " [0, 1493735885],\n",
       " [0, 1493735885],\n",
       " [0, 1493735885],\n",
       " [0, 1493735885],\n",
       " [0, 1493735885],\n",
       " [0, 1493735885],\n",
       " [0, 1493735885],\n",
       " [0, 1493735885],\n",
       " [0, 1493735885],\n",
       " [0, 1493735885],\n",
       " [0, 1493735885],\n",
       " [0, 1493735885],\n",
       " [0, 1493735885],\n",
       " [0, 1493735885],\n",
       " [0, 1493735885],\n",
       " [0, 1493735885],\n",
       " [0, 1493735885],\n",
       " [0, 1493735885],\n",
       " [0, 1493735885],\n",
       " [0, 1493735885],\n",
       " [0, 1493735885],\n",
       " [0, 1493735885],\n",
       " [0, 1493735885],\n",
       " [0, 1493735885],\n",
       " [0, 1493735886],\n",
       " [0, 1493735886],\n",
       " [0, 1493735886],\n",
       " [0, 1493735886],\n",
       " [0, 1493735886],\n",
       " [0, 1493735886],\n",
       " [0, 1493735886],\n",
       " [0, 1493735886],\n",
       " [0, 1493735886],\n",
       " [0, 1493735886],\n",
       " [0, 1493735886],\n",
       " [0, 1493735886],\n",
       " [0, 1493735886],\n",
       " [0, 1493735886],\n",
       " [0, 1493735886],\n",
       " [0, 1493735886],\n",
       " [0, 1493735886],\n",
       " [0, 1493735886],\n",
       " [0, 1493735886],\n",
       " [0, 1493735886],\n",
       " [0, 1493735886],\n",
       " [0, 1493735886],\n",
       " [0, 1493735886],\n",
       " [0, 1493735886],\n",
       " [0, 1493735886],\n",
       " [0, 1493735886],\n",
       " [0, 1493735886],\n",
       " [0, 1493735886],\n",
       " [0, 1493735886],\n",
       " [0, 1493735886],\n",
       " [0, 1493735886],\n",
       " [0, 1493735886],\n",
       " [0, 1493735886],\n",
       " [0, 1493735886],\n",
       " [0, 1493735886],\n",
       " [0, 1493735886],\n",
       " [0, 1493735886],\n",
       " [0, 1493735886],\n",
       " [0, 1493735886],\n",
       " [0, 1493735886],\n",
       " [0, 1493735887],\n",
       " [0, 1493735887],\n",
       " [0, 1493735887],\n",
       " [0, 1493735887],\n",
       " [0, 1493735887],\n",
       " [0, 1493735887],\n",
       " [0, 1493735887],\n",
       " [0, 1493735887],\n",
       " [0, 1493735887],\n",
       " [0, 1493735887],\n",
       " [0, 1493735887],\n",
       " [0, 1493735887],\n",
       " [0, 1493735887],\n",
       " [0, 1493735887],\n",
       " [0, 1493735887],\n",
       " [0, 1493735887],\n",
       " [0, 1493735887],\n",
       " [0, 1493735887],\n",
       " [0, 1493735887],\n",
       " [0, 1493735887],\n",
       " [0, 1493735887],\n",
       " [0, 1493735887],\n",
       " [0, 1493735887],\n",
       " [0, 1493735887],\n",
       " [0, 1493735887],\n",
       " [0, 1493735887],\n",
       " [0, 1493735887],\n",
       " [0, 1493735887],\n",
       " [0, 1493735887],\n",
       " [0, 1493735887],\n",
       " [0, 1493735887],\n",
       " [0, 1493735887],\n",
       " [0, 1493735887],\n",
       " [0, 1493735887],\n",
       " [0, 1493735887],\n",
       " [0, 1493735887],\n",
       " [0, 1493735887],\n",
       " [0, 1493735887],\n",
       " [0, 1493735887],\n",
       " [0, 1493735887],\n",
       " [0, 1493735888],\n",
       " [0, 1493735888],\n",
       " [0, 1493735888],\n",
       " [0, 1493735888],\n",
       " [0, 1493735888],\n",
       " [0, 1493735888],\n",
       " [0, 1493735888],\n",
       " [0, 1493735888],\n",
       " [0, 1493735888],\n",
       " [0, 1493735888],\n",
       " [0, 1493735888],\n",
       " [0, 1493735888],\n",
       " [0, 1493735888],\n",
       " [0, 1493735888],\n",
       " [0, 1493735888],\n",
       " [0, 1493735888],\n",
       " [0, 1493735888],\n",
       " [0, 1493735888],\n",
       " [0, 1493735888],\n",
       " [0, 1493735888],\n",
       " [0, 1493735888],\n",
       " [0, 1493735888],\n",
       " [0, 1493735888],\n",
       " [0, 1493735888],\n",
       " [0, 1493735888],\n",
       " [0, 1493735888],\n",
       " [0, 1493735888],\n",
       " [0, 1493735888],\n",
       " [0, 1493735888],\n",
       " [0, 1493735888],\n",
       " [0, 1493735888],\n",
       " [0, 1493735888],\n",
       " [0, 1493735888],\n",
       " [0, 1493735888],\n",
       " [0, 1493735888],\n",
       " [0, 1493735888],\n",
       " [0, 1493735888],\n",
       " [0, 1493735888],\n",
       " [0, 1493735888],\n",
       " [0, 1493735888],\n",
       " [0, 1493735889],\n",
       " [0, 1493735889],\n",
       " [0, 1493735889],\n",
       " [0, 1493735889],\n",
       " [0, 1493735889],\n",
       " [0, 1493735889],\n",
       " [0, 1493735889],\n",
       " [0, 1493735889],\n",
       " [0, 1493735889],\n",
       " [0, 1493735889],\n",
       " [0, 1493735889],\n",
       " [0, 1493735889],\n",
       " [0, 1493735889],\n",
       " [0, 1493735889],\n",
       " [0, 1493735889],\n",
       " [0, 1493735889],\n",
       " [0, 1493735889],\n",
       " [0, 1493735889],\n",
       " [0, 1493735889],\n",
       " [0, 1493735889],\n",
       " [0, 1493735889],\n",
       " [0, 1493735889],\n",
       " [0, 1493735889],\n",
       " [0, 1493735889],\n",
       " [0, 1493735889],\n",
       " [0, 1493735889],\n",
       " [0, 1493735889],\n",
       " [0, 1493735889],\n",
       " [0, 1493735889],\n",
       " [0, 1493735889],\n",
       " [0, 1493735889],\n",
       " [0, 1493735889],\n",
       " [0, 1493735889],\n",
       " [0, 1493735889],\n",
       " [0, 1493735889],\n",
       " [0, 1493735889],\n",
       " [0, 1493735889],\n",
       " [0, 1493735889],\n",
       " [0, 1493735889],\n",
       " [0, 1493735889],\n",
       " [0, 1493735890],\n",
       " [0, 1493735890],\n",
       " [0, 1493735890],\n",
       " [0, 1493735890],\n",
       " [0, 1493735890],\n",
       " [0, 1493735890],\n",
       " [0, 1493735890],\n",
       " [0, 1493735890],\n",
       " [0, 1493735890],\n",
       " [0, 1493735890],\n",
       " [0, 1493735890],\n",
       " [0, 1493735890],\n",
       " [0, 1493735890],\n",
       " [0, 1493735890],\n",
       " [0, 1493735890],\n",
       " [0, 1493735890],\n",
       " [0, 1493735890],\n",
       " [0, 1493735890],\n",
       " [0, 1493735890],\n",
       " [0, 1493735890],\n",
       " [0, 1493735890],\n",
       " [0, 1493735890],\n",
       " [0, 1493735890],\n",
       " [0, 1493735890],\n",
       " [0, 1493735890],\n",
       " [0, 1493735890],\n",
       " [0, 1493735890],\n",
       " [0, 1493735890],\n",
       " [0, 1493735890],\n",
       " [0, 1493735890],\n",
       " [0, 1493735890],\n",
       " [0, 1493735890],\n",
       " [0, 1493735890],\n",
       " [0, 1493735890],\n",
       " [0, 1493735890],\n",
       " [0, 1493735890],\n",
       " [0, 1493735890],\n",
       " [0, 1493735890],\n",
       " [0, 1493735890],\n",
       " [0, 1493735890],\n",
       " [0, 1493735891],\n",
       " [0, 1493735891],\n",
       " [0, 1493735891],\n",
       " [0, 1493735891],\n",
       " [0, 1493735891],\n",
       " [0, 1493735891],\n",
       " [0, 1493735891],\n",
       " [0, 1493735891],\n",
       " [0, 1493735891],\n",
       " [0, 1493735891],\n",
       " [0, 1493735891],\n",
       " [0, 1493735891],\n",
       " [0, 1493735891],\n",
       " [0, 1493735891],\n",
       " [0, 1493735891],\n",
       " [0, 1493735891],\n",
       " [0, 1493735891],\n",
       " [0, 1493735891],\n",
       " [0, 1493735891],\n",
       " [0, 1493735891],\n",
       " [0, 1493735891],\n",
       " [0, 1493735891],\n",
       " [0, 1493735891],\n",
       " [0, 1493735891],\n",
       " [0, 1493735891],\n",
       " [0, 1493735891],\n",
       " [0, 1493735891],\n",
       " [0, 1493735891],\n",
       " [0, 1493735891],\n",
       " [0, 1493735891],\n",
       " [0, 1493735891],\n",
       " [0, 1493735891],\n",
       " [0, 1493735891],\n",
       " [0, 1493735891],\n",
       " [0, 1493735891],\n",
       " [0, 1493735891],\n",
       " [0, 1493735891],\n",
       " [0, 1493735891],\n",
       " [0, 1493735891],\n",
       " [0, 1493735891],\n",
       " [0, 1493735892],\n",
       " [0, 1493735892],\n",
       " [0, 1493735892],\n",
       " [0, 1493735892],\n",
       " [0, 1493735892],\n",
       " [0, 1493735892],\n",
       " [0, 1493735892],\n",
       " [0, 1493735892],\n",
       " [0, 1493735892],\n",
       " [0, 1493735892],\n",
       " [0, 1493735892],\n",
       " [0, 1493735892],\n",
       " [0, 1493735892],\n",
       " [0, 1493735892],\n",
       " [0, 1493735892],\n",
       " [0, 1493735892],\n",
       " [0, 1493735892],\n",
       " [0, 1493735892],\n",
       " [0, 1493735892],\n",
       " [0, 1493735892],\n",
       " [0, 1493735892],\n",
       " [0, 1493735892],\n",
       " [0, 1493735892],\n",
       " [0, 1493735892],\n",
       " [0, 1493735892],\n",
       " [0, 1493735892],\n",
       " [0, 1493735892],\n",
       " [0, 1493735892],\n",
       " [0, 1493735892],\n",
       " [0, 1493735892],\n",
       " [0, 1493735892],\n",
       " [0, 1493735892],\n",
       " [0, 1493735892],\n",
       " [0, 1493735892],\n",
       " [0, 1493735892],\n",
       " [0, 1493735892],\n",
       " [0, 1493735892],\n",
       " [0, 1493735892],\n",
       " [0, 1493735892],\n",
       " [0, 1493735892],\n",
       " [0, 1493735893],\n",
       " [0, 1493735893],\n",
       " [0, 1493735893],\n",
       " [0, 1493735893],\n",
       " [0, 1493735893],\n",
       " [0, 1493735893],\n",
       " [0, 1493735893],\n",
       " [0, 1493735893],\n",
       " [0, 1493735893],\n",
       " [0, 1493735893],\n",
       " [0, 1493735893],\n",
       " [0, 1493735893],\n",
       " [0, 1493735893],\n",
       " [0, 1493735893],\n",
       " [0, 1493735893],\n",
       " [0, 1493735893],\n",
       " [0, 1493735893],\n",
       " [0, 1493735893],\n",
       " [0, 1493735893],\n",
       " [0, 1493735893],\n",
       " [0, 1493735893],\n",
       " [0, 1493735893],\n",
       " [0, 1493735893],\n",
       " [0, 1493735893],\n",
       " [0, 1493735893],\n",
       " [0, 1493735893],\n",
       " [0, 1493735893],\n",
       " [0, 1493735893],\n",
       " [0, 1493735893],\n",
       " [0, 1493735893],\n",
       " [0, 1493735893],\n",
       " [0, 1493735893],\n",
       " [0, 1493735893],\n",
       " [0, 1493735893],\n",
       " [0, 1493735893],\n",
       " [0, 1493735893],\n",
       " [0, 1493735893],\n",
       " [0, 1493735893],\n",
       " [0, 1493735893],\n",
       " [0, 1493735893],\n",
       " [0, 1493735894],\n",
       " [0, 1493735894],\n",
       " [0, 1493735894],\n",
       " [0, 1493735894],\n",
       " [0, 1493735894],\n",
       " [0, 1493735894],\n",
       " [0, 1493735894],\n",
       " [0, 1493735894],\n",
       " [0, 1493735894],\n",
       " [0, 1493735894],\n",
       " [0, 1493735894],\n",
       " [0, 1493735894],\n",
       " [0, 1493735894],\n",
       " [0, 1493735894],\n",
       " [0, 1493735894],\n",
       " [0, 1493735894],\n",
       " [0, 1493735894],\n",
       " [0, 1493735894],\n",
       " [0, 1493735894],\n",
       " [0, 1493735894],\n",
       " [0, 1493735894],\n",
       " [0, 1493735894],\n",
       " [0, 1493735894],\n",
       " [0, 1493735894],\n",
       " [0, 1493735894],\n",
       " [0, 1493735894],\n",
       " [0, 1493735894],\n",
       " [0, 1493735894],\n",
       " [0, 1493735894],\n",
       " [0, 1493735894],\n",
       " [0, 1493735894],\n",
       " [0, 1493735894],\n",
       " [0, 1493735894],\n",
       " [0, 1493735894],\n",
       " [0, 1493735894],\n",
       " [0, 1493735894],\n",
       " [0, 1493735894],\n",
       " [0, 1493735894],\n",
       " [0, 1493735894],\n",
       " [0, 1493735894],\n",
       " [0, 1493735895],\n",
       " [0, 1493735895],\n",
       " [0, 1493735895],\n",
       " [0, 1493735895],\n",
       " [0, 1493735895],\n",
       " [0, 1493735895],\n",
       " [0, 1493735895],\n",
       " [0, 1493735895],\n",
       " [0, 1493735895],\n",
       " [0, 1493735895],\n",
       " [0, 1493735895],\n",
       " [0, 1493735895],\n",
       " [0, 1493735895],\n",
       " [0, 1493735895],\n",
       " [0, 1493735895],\n",
       " [0, 1493735895],\n",
       " [0, 1493735895],\n",
       " [0, 1493735895],\n",
       " [0, 1493735895],\n",
       " [0, 1493735895],\n",
       " [0, 1493735895],\n",
       " [0, 1493735895],\n",
       " [0, 1493735895],\n",
       " [0, 1493735895],\n",
       " [0, 1493735895],\n",
       " [0, 1493735895],\n",
       " ...]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Up sampling tac data to match acc data\n",
    "clean_ts = clean_tac_data['timestamp'] \n",
    "acc_ts = acc_data_pid['time']\n",
    "all_labels = list()\n",
    "offset_tac, offset_acc = 0, 0\n",
    "# print(acc_ts.iloc[0])\n",
    "# print(clean_ts.loc[0])\n",
    "# print(clean_tac_data.loc[0]['TAC_Reading'])\n",
    "# # acc_ts.iloc[0] #1493735870653\n",
    "while offset_tac < len(clean_ts) and offset_acc < len(acc_ts):\n",
    "  \n",
    "  while acc_ts.iloc[offset_acc] < clean_ts.iloc[offset_tac]:\n",
    "    all_labels.append([clean_tac_data.iloc[offset_tac]['TAC_Reading'], acc_ts.iloc[offset_acc]])\n",
    "    offset_acc += 1\n",
    "    if offset_acc >= len(acc_ts):\n",
    "      break\n",
    "\n",
    "  offset_tac += 1\n",
    "\n",
    "all_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1225727, 2), (1225727, 5))"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_labels_df = pd.DataFrame(all_labels, columns = [\"tac\", \"time\"])\n",
    "all_labels_df.shape, acc_data_pid.shape\n",
    "# merged = merged.drop_duplicates().reset_index(drop=True)\n",
    "# merged.to_csv(\"../data/BK7610_final_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged = acc_data_pid.head(10).merge(all_labels_df.head(10), on = 'time', how='inner')\n",
    "# merged['time'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(all_labels_df['time'].unique()), len(acc_data_pid['time'].unique())\n",
    "clean_tac_data[\"timestamp\"].is_monotonic_increasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acc_data_pid['tac_reading'] = \n",
    "# TODO: Make sure tac data is sorted on timestamp\n",
    "clean_tac_data[\"from\"] = clean_tac_data[\"timestamp\"].shift(1, fill_value=-1) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_tac_data.index = pd.IntervalIndex.from_arrays(clean_tac_data[\"from\"], clean_tac_data[\"timestamp\"], closed = \"both\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7y/kyw1v_8j0g1ckfb3g6x93q1m0000gn/T/ipykernel_15281/2638206493.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  acc_data_pid['tac'] = acc_data_pid[\"time\"].apply(lambda x: clean_tac_data.iloc[clean_tac_data.index.get_loc(x)][\"TAC_Reading\"])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pid</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>time</th>\n",
       "      <th>tac</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>47136</th>\n",
       "      <td>BK7610</td>\n",
       "      <td>0.1261</td>\n",
       "      <td>-0.0078</td>\n",
       "      <td>-0.0243</td>\n",
       "      <td>1493735870</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47138</th>\n",
       "      <td>BK7610</td>\n",
       "      <td>0.1336</td>\n",
       "      <td>-0.0697</td>\n",
       "      <td>-0.0446</td>\n",
       "      <td>1493735870</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47140</th>\n",
       "      <td>BK7610</td>\n",
       "      <td>0.1443</td>\n",
       "      <td>-0.0474</td>\n",
       "      <td>-0.0447</td>\n",
       "      <td>1493735870</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47142</th>\n",
       "      <td>BK7610</td>\n",
       "      <td>0.1255</td>\n",
       "      <td>-0.0038</td>\n",
       "      <td>0.0111</td>\n",
       "      <td>1493735870</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47144</th>\n",
       "      <td>BK7610</td>\n",
       "      <td>0.1076</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>0.0276</td>\n",
       "      <td>1493735870</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6071104</th>\n",
       "      <td>BK7610</td>\n",
       "      <td>-0.0784</td>\n",
       "      <td>-0.0161</td>\n",
       "      <td>0.1719</td>\n",
       "      <td>1493767770</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6071108</th>\n",
       "      <td>BK7610</td>\n",
       "      <td>-0.0395</td>\n",
       "      <td>-0.0816</td>\n",
       "      <td>0.1634</td>\n",
       "      <td>1493767770</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6071112</th>\n",
       "      <td>BK7610</td>\n",
       "      <td>0.0160</td>\n",
       "      <td>-0.0853</td>\n",
       "      <td>0.0906</td>\n",
       "      <td>1493767770</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6071117</th>\n",
       "      <td>BK7610</td>\n",
       "      <td>0.0901</td>\n",
       "      <td>-0.0767</td>\n",
       "      <td>0.0162</td>\n",
       "      <td>1493767770</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6071121</th>\n",
       "      <td>BK7610</td>\n",
       "      <td>0.1700</td>\n",
       "      <td>-0.1390</td>\n",
       "      <td>-0.0773</td>\n",
       "      <td>1493767770</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1225727 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            pid       x       y       z        time  tac\n",
       "47136    BK7610  0.1261 -0.0078 -0.0243  1493735870    0\n",
       "47138    BK7610  0.1336 -0.0697 -0.0446  1493735870    0\n",
       "47140    BK7610  0.1443 -0.0474 -0.0447  1493735870    0\n",
       "47142    BK7610  0.1255 -0.0038  0.0111  1493735870    0\n",
       "47144    BK7610  0.1076  0.0032  0.0276  1493735870    0\n",
       "...         ...     ...     ...     ...         ...  ...\n",
       "6071104  BK7610 -0.0784 -0.0161  0.1719  1493767770    1\n",
       "6071108  BK7610 -0.0395 -0.0816  0.1634  1493767770    1\n",
       "6071112  BK7610  0.0160 -0.0853  0.0906  1493767770    1\n",
       "6071117  BK7610  0.0901 -0.0767  0.0162  1493767770    1\n",
       "6071121  BK7610  0.1700 -0.1390 -0.0773  1493767770    1\n",
       "\n",
       "[1225727 rows x 6 columns]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_data_pid['tac'] = acc_data_pid[\"time\"].apply(lambda x: clean_tac_data.iloc[clean_tac_data.index.get_loc(x)][\"TAC_Reading\"])\n",
    "acc_data_pid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47136, 6071121)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(acc_data_pid[\"tac\"].unique().sort())\n",
    "# clean_tac_data\n",
    "# min: 1,493,718,714\n",
    "# max: 1,493,807,899\n",
    "# \n",
    "# acc_data_pid\n",
    "# min: 1,493,735,870\n",
    "# max: 1,493,767,770\n",
    "# acc_data_pid[\"time\"].max()\n",
    "# groups = acc_data_pid.groupby([\"time\"])\n",
    "# print(groups.apply(lambda x: x[\"tac\"]<20))\n",
    "# time_stamps = groups.apply(lambda x: x[\"tac\"]<20)\n",
    "# time_stamps \n",
    "\n",
    "min(acc_data_pid.index), max(acc_data_pid.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Make n = 10 after either removing one record which has 7 records for a second or by adding 3 dummy values to it (latter is better)\n",
    "# frame_temp.groupby([ \"pid\", \"window10\"]).count().describe()\n",
    "# We are sampling with replacement, which should be okay since it is within a second\n",
    "acc_data_pid_20s = acc_data_pid.groupby([ \"pid\", \"time\"]).sample(n = 20, replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Make this as an assert statement in the begininng for both tac and accelerometer data\n",
    "acc_data_pid_20s[\"time\"].is_monotonic_increasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SLIDING WINDOW\n",
    "acc_data_sliding = acc_data_pid_20s.copy()\n",
    "# cols = [\"x\", \"y\", \"z\"]\n",
    "# window_size = 10 # including current\n",
    "\n",
    "# for col in cols:\n",
    "#     cols_to_append = []\n",
    "#     for i in range(0, window_size):\n",
    "#         shifted_col_name = str(col) + \"_\" + str(i)\n",
    "#         acc_data_sliding[shifted_col_name] = acc_data_sliding[col].shift(i, fill_value = 0)\n",
    "#         cols_to_append.append(shifted_col_name)\n",
    "    \n",
    "#     # we have (windpw_size ) columsn for col\n",
    "#     # Uncomment to keep original columns\n",
    "#     acc_data_sliding = acc_data_sliding.drop(columns=[col])    \n",
    "#     acc_data_sliding[str(col)] = acc_data_sliding[cols_to_append].values.tolist()\n",
    "    \n",
    "#     # acc_data_sliding = acc_data_sliding.drop(columns=cols_to_append)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # x_sliding_window.shape\n",
    "pids = [\"BK7610\"]\n",
    "final = []\n",
    "labels = []\n",
    "for pid in pids:\n",
    "  # temptemp = acc_data_pid_20s[acc_data_pid_20s['pid'] == pid]\n",
    "  temptemp = acc_data_sliding[acc_data_sliding['pid'] == pid]\n",
    "  times = temptemp.time.unique()\n",
    "  final_temp =[]\n",
    "  labels_temp = []\n",
    "  for i in range(len(times)):\n",
    "    # x = np.lib.stride_tricks.sliding_window_view(frame_temp2[frame_temp2.pid == pid and frame_temp2.window10 == time], window_shape = 10)\n",
    "    # temptemptemp = temptemp[temptemp['time'] == time]\n",
    "    # time_to_filter = [times[j] if j >= 0 else -1 for j in range(i-9, i+1)]\n",
    "    time_to_filter = [times[j] if j >= 0 else -1 for j in range(i-9, i+1)] # Create sliding window \n",
    "    # print(time_to_filter)\n",
    "    # if i == 10:\n",
    "    #   break\n",
    "    temptemptemp = temptemp[temptemp['time'].isin(time_to_filter)]\n",
    "    # TODO: Create x y z sliding windows\n",
    "    # x = np.lib.stride_tricks.sliding_window_view(temptemptemp[\"x\"], window_shape = 10)\n",
    "    # y = np.lib.stride_tricks.sliding_window_view(temptemptemp[\"y\"], window_shape = 10)\n",
    "    # z = np.lib.stride_tricks.sliding_window_view(temptemptemp[\"z\"], window_shape = 10)\n",
    "\n",
    "    # x_dash = np.array(temptemptemp[[\"x_\"+str(i) for i in range(window_size)]]).flatten()\n",
    "    # y_dash = np.array(temptemptemp[[\"y_\"+str(i) for i in range(window_size)]]).flatten()\n",
    "    # z_dash = np.array(temptemptemp[[\"z_\"+str(i) for i in range(window_size)]]).flatten()\n",
    "\n",
    "    x_dash = np.array(temptemptemp[\"x\"])\n",
    "    y_dash = np.array(temptemptemp[\"y\"])\n",
    "    z_dash = np.array(temptemptemp[\"z\"])\n",
    "\n",
    "    x_dash = np.pad(x_dash, (200 - len(x_dash), 0), \"constant\")\n",
    "    y_dash = np.pad(y_dash, (200 - len(y_dash), 0), \"constant\")\n",
    "    z_dash = np.pad(z_dash, (200 - len(z_dash), 0), \"constant\")\n",
    "\n",
    "    # a = np.vstack((temptemptemp[\"x\"].apply(lambda x: np.array(x, dtype=\"float32\")), temptemptemp[\"y\"].apply(lambda x: np.array(x, dtype=\"float32\")), temptemptemp[\"z\"].apply(lambda x: np.array(x, dtype=\"float32\"))))\n",
    "    a = np.transpose(np.vstack((x_dash, y_dash, z_dash)))\n",
    "    final_temp.append(a)\n",
    "    labels_temp.append(temptemptemp.head(1)[\"tac\"])\n",
    "  final.append(np.array(final_temp))\n",
    "  labels.append(np.array(labels_temp))\n",
    "  # print(final)\n",
    "  \n",
    "  # break\n",
    "# print(np.array(final,dtype=object).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temptemptemp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 3)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [-0.1331,  0.026 ,  0.1165, -0.0039, -0.1326, -0.1095,  0.1165,\n",
    "        #  0.1467, -0.1494, -0.0233]\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 30735, 200, 3)\n",
      "(1, 30735, 1)\n"
     ]
    }
   ],
   "source": [
    "final_arr = np.asarray(final).astype('float32')\n",
    "\n",
    "# final_arr = np.reshape(final_arr, (final_arr.shape[0], final_arr.shape[1], final_arr.shape[3], final_arr.shape[2]))\n",
    "\n",
    "labels_arr = np.asarray(labels).astype('float32')\n",
    "print(final_arr.shape)\n",
    "print(labels_arr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_arr[0][9][0:19] == final_arr[0][0][180:199]\n",
    "# np.unique(labels_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((30735, 200, 3), (30735, 1))"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_arr_reshape = np.reshape(final_arr, (30735, 200, 3)) \n",
    "labels_arr_reshape = np.reshape(labels_arr, (30735,1))\n",
    "final_arr_reshape.shape, labels_arr_reshape.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = range(len(final_arr_reshape))\n",
    "indices = tf.random.shuffle(indices)\n",
    "\n",
    "final_arr_reshape = tf.gather(final_arr_reshape, indices)\n",
    "labels_arr_reshape = tf.gather(labels_arr_reshape, indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from TF_Model import Drunk\n",
    "# drunk = Drunk()\n",
    "\n",
    "# drunk.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=False), \\\n",
    "#                    optimizer=tf.keras.optimizers.Adam(0.03), \\\n",
    "#                    metrics=[tf.keras.metrics.Accuracy()]) \n",
    "\n",
    "# drunk.build((30735, 600))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model \n",
    "#### 3 hidden layers (32 nodes, RELU, 32 nodes, RELU, 16 nodes, RELU)  \n",
    "#### output layer uses sigmoid activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_data = np.reshape(final_arr, (30735, 400, 3)) \n",
    "base_model_labels = np.reshape(labels_arr, (30735,1))\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 50\n",
    "flatten = tf.keras.layers.Flatten()\n",
    "# fc_layer1 = tf.keras.layers.Dense(units=1024, activation = 'relu')\n",
    "# fc_layer2 = tf.keras.layers.Dense(units=768, activation = 'relu')\n",
    "# fc_layer3 = tf.keras.layers.Dense(units=512, activation = 'relu')\n",
    "# fc_layer4 = tf.keras.layers.Dense(units=256, activation = 'relu')\n",
    "fc_layer5 = tf.keras.layers.Dense(units=128, activation = 'relu')\n",
    "fc_layer6 = tf.keras.layers.Dense(units=64, activation = 'relu')\n",
    "fc_layer7 = tf.keras.layers.Dense(units=32, activation = 'relu')\n",
    "fc_layer8 = tf.keras.layers.Dense(units=1, activation = 'sigmoid')\n",
    "# Cant use softmax at the end since it will normalize and give 1 \n",
    "base_model = tf.keras.Sequential([\n",
    "    flatten,\n",
    "#  fc_layer1, fc_layer2, \n",
    "# fc_layer3, \n",
    "# fc_layer4,\n",
    "fc_layer5,\n",
    "fc_layer6,\n",
    "fc_layer7,\n",
    "fc_layer8\n",
    "])\n",
    "\n",
    "base_model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits = False), \\\n",
    "                   optimizer=tf.keras.optimizers.Adam(0.001, beta_1=0.9, beta_2= 0.999), \\\n",
    "                   metrics=[tf.keras.metrics.Accuracy()]) \n",
    "\n",
    "base_model.build((30735, 400, 3))\n",
    "base_model.summary() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=100)\n",
    "base_model.fit(base_model_data, base_model_labels, epochs = 1000,\n",
    "               batch_size = 256, \n",
    "               verbose=1, \n",
    "               callbacks=[callback])  \n",
    "\n",
    "# Paper baseline model as-it-is\n",
    "#                Epoch 500/500\n",
    "# 961/961 [==============================] - 1s 579us/step - loss: 0.0989 - accuracy: 0.4819"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.00033186]], dtype=float32)>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# base_model_data[0:32]\n",
    "base_model(base_model_data[0:1])\n",
    "# base_model_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = base_model.evaluate(base_model_data, base_model_labels,\n",
    "                                    #  batch_size = batch_size, \n",
    "                                     verbose=0)\n",
    "\n",
    "round(loss,4),accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "epochs = 1000\n",
    "# prep_data = pd.read_csv('../data/good_again_bhas.csv')\n",
    "flatten = tf.keras.layers.Flatten()\n",
    "conv_layer1 = tf.keras.layers.Conv1D(filters = 64, kernel_size = 3, padding='SAME')\n",
    "conv_layer2 = tf.keras.layers.Conv1D(filters = 64, kernel_size = 3, padding='SAME')\n",
    "dropout = tf.keras.layers.Dropout(0.5)\n",
    "max_pooling = tf.keras.layers.MaxPool1D(pool_size=2)\n",
    "# fc - fully connected layer\n",
    "fc_layer = tf.keras.layers.Dense(units=128, activation = 'relu')\n",
    "fc_layer2 = tf.keras.layers.Dense(units=1, activation = 'sigmoid') \n",
    "base_model = tf.keras.Sequential([\n",
    "                                  conv_layer1,  \n",
    "                                  conv_layer2, \n",
    "                                  dropout, \n",
    "                                  max_pooling, \n",
    "                                  flatten, \n",
    "                                  fc_layer, \n",
    "                                  fc_layer2\n",
    "                                ])\n",
    "\n",
    "# base_model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=False, label_smoothing=0.0001), \\\n",
    "base_model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=False), \\\n",
    "                   optimizer=tf.keras.optimizers.Adam(0.001), \\\n",
    "                   metrics=[tf.keras.metrics.Accuracy()]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_func = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "# optimizer = tf.keras.optimizers.Adam(0.001)\n",
    "\n",
    "# for e in range(0, 5000):\n",
    "#     with tf.GradientTape() as tape:\n",
    "#         preds = base_model(final_arr_reshape)\n",
    "#         print(preds)\n",
    "        \n",
    "#         # loss = loss_func(y_true=labels_arr_reshape, y_pred=preds)\n",
    "#         # acc = np.sum(np.equal(labels, preds)) / 30735\n",
    "#     #     print(acc)\n",
    "#     # gradients = tape.gradient(loss, base_model.trainable_variables)\n",
    "#     # optimizer.apply_gradients(zip(gradients, base_model.trainable_variables))\n",
    "#     # print(f\"Epoch: {e} | LOSS : {loss} | acc {acc}\")\n",
    "\n",
    "# # loss\n",
    "            \n",
    "# # total_loss += loss\n",
    "# # gradients = tape.gradient(loss, model.trainable_variables)\n",
    "# #     optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "# #     return total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_4 (Conv1D)           (30735, 400, 64)          640       \n",
      "                                                                 \n",
      " conv1d_5 (Conv1D)           (30735, 400, 64)          12352     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (30735, 400, 64)          0         \n",
      "                                                                 \n",
      " max_pooling1d_2 (MaxPooling  (30735, 200, 64)         0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " flatten_6 (Flatten)         (30735, 12800)            0         \n",
      "                                                                 \n",
      " dense_22 (Dense)            (30735, 128)              1638528   \n",
      "                                                                 \n",
      " dense_23 (Dense)            (30735, 1)                129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,651,649\n",
      "Trainable params: 1,651,649\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "base_model.build((30735, 400, 3))\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "121/121 [==============================] - 7s 53ms/step - loss: 0.6184 - accuracy: 0.0000e+00\n",
      "Epoch 2/1000\n",
      "121/121 [==============================] - 6s 53ms/step - loss: 0.5738 - accuracy: 0.0000e+00\n",
      "Epoch 3/1000\n",
      "121/121 [==============================] - 6s 53ms/step - loss: 0.5513 - accuracy: 0.0000e+00\n",
      "Epoch 4/1000\n",
      "121/121 [==============================] - 6s 53ms/step - loss: 0.5321 - accuracy: 0.0000e+00\n",
      "Epoch 5/1000\n",
      "121/121 [==============================] - 6s 53ms/step - loss: 0.5135 - accuracy: 0.0000e+00\n",
      "Epoch 6/1000\n",
      "121/121 [==============================] - 6s 53ms/step - loss: 0.4935 - accuracy: 0.0000e+00\n",
      "Epoch 7/1000\n",
      "121/121 [==============================] - 6s 53ms/step - loss: 0.4826 - accuracy: 0.0000e+00\n",
      "Epoch 8/1000\n",
      "121/121 [==============================] - 6s 53ms/step - loss: 0.4593 - accuracy: 1.3014e-04\n",
      "Epoch 9/1000\n",
      "121/121 [==============================] - 6s 53ms/step - loss: 0.4389 - accuracy: 2.6029e-04\n",
      "Epoch 10/1000\n",
      "121/121 [==============================] - 6s 53ms/step - loss: 0.4232 - accuracy: 0.0011\n",
      "Epoch 11/1000\n",
      "121/121 [==============================] - 6s 53ms/step - loss: 0.3966 - accuracy: 0.0020\n",
      "Epoch 12/1000\n",
      "121/121 [==============================] - 7s 56ms/step - loss: 0.3823 - accuracy: 0.0047\n",
      "Epoch 13/1000\n",
      "121/121 [==============================] - 7s 54ms/step - loss: 0.3572 - accuracy: 0.0067\n",
      "Epoch 14/1000\n",
      "121/121 [==============================] - 6s 53ms/step - loss: 0.3382 - accuracy: 0.0099\n",
      "Epoch 15/1000\n",
      "121/121 [==============================] - 7s 54ms/step - loss: 0.3251 - accuracy: 0.0135\n",
      "Epoch 16/1000\n",
      "121/121 [==============================] - 7s 54ms/step - loss: 0.3111 - accuracy: 0.0181\n",
      "Epoch 17/1000\n",
      "121/121 [==============================] - 7s 54ms/step - loss: 0.2979 - accuracy: 0.0189\n",
      "Epoch 18/1000\n",
      "121/121 [==============================] - 6s 54ms/step - loss: 0.2807 - accuracy: 0.0228\n",
      "Epoch 19/1000\n",
      "121/121 [==============================] - 7s 54ms/step - loss: 0.2726 - accuracy: 0.0286\n",
      "Epoch 20/1000\n",
      "121/121 [==============================] - 7s 54ms/step - loss: 0.2630 - accuracy: 0.0311\n",
      "Epoch 21/1000\n",
      "121/121 [==============================] - 7s 54ms/step - loss: 0.2502 - accuracy: 0.0379\n",
      "Epoch 22/1000\n",
      "121/121 [==============================] - 7s 54ms/step - loss: 0.2439 - accuracy: 0.0448\n",
      "Epoch 23/1000\n",
      "121/121 [==============================] - 7s 54ms/step - loss: 0.2350 - accuracy: 0.0511\n",
      "Epoch 24/1000\n",
      "121/121 [==============================] - 7s 54ms/step - loss: 0.2326 - accuracy: 0.0524\n",
      "Epoch 25/1000\n",
      "121/121 [==============================] - 7s 54ms/step - loss: 0.2222 - accuracy: 0.0567\n",
      "Epoch 26/1000\n",
      "121/121 [==============================] - 7s 54ms/step - loss: 0.2068 - accuracy: 0.0640\n",
      "Epoch 27/1000\n",
      "121/121 [==============================] - 7s 54ms/step - loss: 0.2034 - accuracy: 0.0728\n",
      "Epoch 28/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.1992 - accuracy: 0.0757\n",
      "Epoch 29/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.2022 - accuracy: 0.0810\n",
      "Epoch 30/1000\n",
      "121/121 [==============================] - 7s 54ms/step - loss: 0.2009 - accuracy: 0.0796\n",
      "Epoch 31/1000\n",
      "121/121 [==============================] - 7s 54ms/step - loss: 0.1822 - accuracy: 0.0823\n",
      "Epoch 32/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.1887 - accuracy: 0.0921\n",
      "Epoch 33/1000\n",
      "121/121 [==============================] - 7s 54ms/step - loss: 0.1780 - accuracy: 0.0939\n",
      "Epoch 34/1000\n",
      "121/121 [==============================] - 7s 54ms/step - loss: 0.1785 - accuracy: 0.1024\n",
      "Epoch 35/1000\n",
      "121/121 [==============================] - 7s 54ms/step - loss: 0.1800 - accuracy: 0.1054\n",
      "Epoch 36/1000\n",
      "121/121 [==============================] - 7s 56ms/step - loss: 0.1694 - accuracy: 0.1135\n",
      "Epoch 37/1000\n",
      "121/121 [==============================] - 7s 54ms/step - loss: 0.1782 - accuracy: 0.1133\n",
      "Epoch 38/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.1679 - accuracy: 0.1115\n",
      "Epoch 39/1000\n",
      "121/121 [==============================] - 7s 54ms/step - loss: 0.1723 - accuracy: 0.1201\n",
      "Epoch 40/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.1694 - accuracy: 0.1235\n",
      "Epoch 41/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.1633 - accuracy: 0.1267\n",
      "Epoch 42/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.1765 - accuracy: 0.1218\n",
      "Epoch 43/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.1606 - accuracy: 0.1251\n",
      "Epoch 44/1000\n",
      "121/121 [==============================] - 7s 54ms/step - loss: 0.1490 - accuracy: 0.1336\n",
      "Epoch 45/1000\n",
      "121/121 [==============================] - 7s 54ms/step - loss: 0.1412 - accuracy: 0.1405\n",
      "Epoch 46/1000\n",
      "121/121 [==============================] - 7s 54ms/step - loss: 0.1438 - accuracy: 0.1430\n",
      "Epoch 47/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.1490 - accuracy: 0.1402\n",
      "Epoch 48/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.1465 - accuracy: 0.1431\n",
      "Epoch 49/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.1319 - accuracy: 0.1573\n",
      "Epoch 50/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.1358 - accuracy: 0.1545\n",
      "Epoch 51/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.1228 - accuracy: 0.1539\n",
      "Epoch 52/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.1309 - accuracy: 0.1589\n",
      "Epoch 53/1000\n",
      "121/121 [==============================] - 7s 54ms/step - loss: 0.1189 - accuracy: 0.1556\n",
      "Epoch 54/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.1219 - accuracy: 0.1669\n",
      "Epoch 55/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.1145 - accuracy: 0.1799\n",
      "Epoch 56/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.1182 - accuracy: 0.1799\n",
      "Epoch 57/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.1180 - accuracy: 0.1716\n",
      "Epoch 58/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.1162 - accuracy: 0.1750\n",
      "Epoch 59/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.1133 - accuracy: 0.1803\n",
      "Epoch 60/1000\n",
      "121/121 [==============================] - 7s 54ms/step - loss: 0.1065 - accuracy: 0.1818\n",
      "Epoch 61/1000\n",
      "121/121 [==============================] - 7s 54ms/step - loss: 0.1064 - accuracy: 0.1863\n",
      "Epoch 62/1000\n",
      "121/121 [==============================] - 7s 54ms/step - loss: 0.1016 - accuracy: 0.1841\n",
      "Epoch 63/1000\n",
      "121/121 [==============================] - 7s 54ms/step - loss: 0.1032 - accuracy: 0.1894\n",
      "Epoch 64/1000\n",
      "121/121 [==============================] - 7s 54ms/step - loss: 0.0962 - accuracy: 0.1972\n",
      "Epoch 65/1000\n",
      "121/121 [==============================] - 7s 54ms/step - loss: 0.0998 - accuracy: 0.1972\n",
      "Epoch 66/1000\n",
      "121/121 [==============================] - 7s 54ms/step - loss: 0.1009 - accuracy: 0.1984\n",
      "Epoch 67/1000\n",
      "121/121 [==============================] - 7s 54ms/step - loss: 0.1004 - accuracy: 0.2045\n",
      "Epoch 68/1000\n",
      "121/121 [==============================] - 7s 54ms/step - loss: 0.0921 - accuracy: 0.2004\n",
      "Epoch 69/1000\n",
      "121/121 [==============================] - 7s 54ms/step - loss: 0.1047 - accuracy: 0.1959\n",
      "Epoch 70/1000\n",
      "121/121 [==============================] - 7s 54ms/step - loss: 0.1037 - accuracy: 0.2005\n",
      "Epoch 71/1000\n",
      "121/121 [==============================] - 7s 54ms/step - loss: 0.0916 - accuracy: 0.2071\n",
      "Epoch 72/1000\n",
      "121/121 [==============================] - 7s 54ms/step - loss: 0.1113 - accuracy: 0.2085\n",
      "Epoch 73/1000\n",
      "121/121 [==============================] - 7s 54ms/step - loss: 0.0870 - accuracy: 0.2126\n",
      "Epoch 74/1000\n",
      "121/121 [==============================] - 7s 54ms/step - loss: 0.0937 - accuracy: 0.2116\n",
      "Epoch 75/1000\n",
      "121/121 [==============================] - 7s 54ms/step - loss: 0.0911 - accuracy: 0.2092\n",
      "Epoch 76/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0833 - accuracy: 0.2097\n",
      "Epoch 77/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0882 - accuracy: 0.2204\n",
      "Epoch 78/1000\n",
      "121/121 [==============================] - 7s 54ms/step - loss: 0.0868 - accuracy: 0.2138\n",
      "Epoch 79/1000\n",
      "121/121 [==============================] - 7s 54ms/step - loss: 0.0981 - accuracy: 0.2152\n",
      "Epoch 80/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.1030 - accuracy: 0.2169\n",
      "Epoch 81/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0818 - accuracy: 0.2248\n",
      "Epoch 82/1000\n",
      "121/121 [==============================] - 7s 54ms/step - loss: 0.1118 - accuracy: 0.2213\n",
      "Epoch 83/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0867 - accuracy: 0.2176\n",
      "Epoch 84/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0804 - accuracy: 0.2221\n",
      "Epoch 85/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0805 - accuracy: 0.2236\n",
      "Epoch 86/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0806 - accuracy: 0.2233\n",
      "Epoch 87/1000\n",
      "121/121 [==============================] - 7s 54ms/step - loss: 0.0886 - accuracy: 0.2252\n",
      "Epoch 88/1000\n",
      "121/121 [==============================] - 7s 54ms/step - loss: 0.0798 - accuracy: 0.2252\n",
      "Epoch 89/1000\n",
      "121/121 [==============================] - 7s 54ms/step - loss: 0.0829 - accuracy: 0.2263\n",
      "Epoch 90/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0774 - accuracy: 0.2320\n",
      "Epoch 91/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0752 - accuracy: 0.2370\n",
      "Epoch 92/1000\n",
      "121/121 [==============================] - 7s 54ms/step - loss: 0.0882 - accuracy: 0.2293\n",
      "Epoch 93/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0751 - accuracy: 0.2343\n",
      "Epoch 94/1000\n",
      "121/121 [==============================] - 7s 54ms/step - loss: 0.0755 - accuracy: 0.2365\n",
      "Epoch 95/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0743 - accuracy: 0.2366\n",
      "Epoch 96/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0762 - accuracy: 0.2380\n",
      "Epoch 97/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0831 - accuracy: 0.2447\n",
      "Epoch 98/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0816 - accuracy: 0.2356\n",
      "Epoch 99/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0884 - accuracy: 0.2392\n",
      "Epoch 100/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0825 - accuracy: 0.2384\n",
      "Epoch 101/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0710 - accuracy: 0.2398\n",
      "Epoch 102/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0756 - accuracy: 0.2482\n",
      "Epoch 103/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0799 - accuracy: 0.2378\n",
      "Epoch 104/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0719 - accuracy: 0.2391\n",
      "Epoch 105/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0757 - accuracy: 0.2468\n",
      "Epoch 106/1000\n",
      "121/121 [==============================] - 7s 54ms/step - loss: 0.0681 - accuracy: 0.2517\n",
      "Epoch 107/1000\n",
      "121/121 [==============================] - 7s 54ms/step - loss: 0.0680 - accuracy: 0.2500\n",
      "Epoch 108/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0860 - accuracy: 0.2487\n",
      "Epoch 109/1000\n",
      "121/121 [==============================] - 7s 54ms/step - loss: 0.0766 - accuracy: 0.2451\n",
      "Epoch 110/1000\n",
      "121/121 [==============================] - 7s 54ms/step - loss: 0.0699 - accuracy: 0.2494\n",
      "Epoch 111/1000\n",
      "121/121 [==============================] - 7s 54ms/step - loss: 0.0697 - accuracy: 0.2549\n",
      "Epoch 112/1000\n",
      "121/121 [==============================] - 7s 54ms/step - loss: 0.0716 - accuracy: 0.2552\n",
      "Epoch 113/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0714 - accuracy: 0.2520\n",
      "Epoch 114/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0791 - accuracy: 0.2497\n",
      "Epoch 115/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0731 - accuracy: 0.2509\n",
      "Epoch 116/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0678 - accuracy: 0.2544\n",
      "Epoch 117/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0668 - accuracy: 0.2580\n",
      "Epoch 118/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0724 - accuracy: 0.2518\n",
      "Epoch 119/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0643 - accuracy: 0.2534\n",
      "Epoch 120/1000\n",
      "121/121 [==============================] - 7s 59ms/step - loss: 0.0692 - accuracy: 0.2547\n",
      "Epoch 121/1000\n",
      "121/121 [==============================] - 7s 56ms/step - loss: 0.0788 - accuracy: 0.2666\n",
      "Epoch 122/1000\n",
      "121/121 [==============================] - 7s 57ms/step - loss: 0.0626 - accuracy: 0.2664\n",
      "Epoch 123/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0700 - accuracy: 0.2611\n",
      "Epoch 124/1000\n",
      "121/121 [==============================] - 7s 54ms/step - loss: 0.0780 - accuracy: 0.2597\n",
      "Epoch 125/1000\n",
      "121/121 [==============================] - 7s 54ms/step - loss: 0.0681 - accuracy: 0.2686\n",
      "Epoch 126/1000\n",
      "121/121 [==============================] - 7s 57ms/step - loss: 0.0730 - accuracy: 0.2634\n",
      "Epoch 127/1000\n",
      "121/121 [==============================] - 8s 66ms/step - loss: 0.0714 - accuracy: 0.2652\n",
      "Epoch 128/1000\n",
      "121/121 [==============================] - 7s 56ms/step - loss: 0.0661 - accuracy: 0.2699\n",
      "Epoch 129/1000\n",
      "121/121 [==============================] - 7s 56ms/step - loss: 0.0682 - accuracy: 0.2753\n",
      "Epoch 130/1000\n",
      "121/121 [==============================] - 7s 58ms/step - loss: 0.0611 - accuracy: 0.2732\n",
      "Epoch 131/1000\n",
      "121/121 [==============================] - 7s 61ms/step - loss: 0.0649 - accuracy: 0.2653\n",
      "Epoch 132/1000\n",
      "121/121 [==============================] - 7s 59ms/step - loss: 0.0661 - accuracy: 0.2691\n",
      "Epoch 133/1000\n",
      "121/121 [==============================] - 7s 57ms/step - loss: 0.0661 - accuracy: 0.2749\n",
      "Epoch 134/1000\n",
      "121/121 [==============================] - 7s 60ms/step - loss: 0.0816 - accuracy: 0.2721\n",
      "Epoch 135/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0690 - accuracy: 0.2748\n",
      "Epoch 136/1000\n",
      "121/121 [==============================] - 7s 54ms/step - loss: 0.0712 - accuracy: 0.2675\n",
      "Epoch 137/1000\n",
      "121/121 [==============================] - 7s 57ms/step - loss: 0.0680 - accuracy: 0.2752\n",
      "Epoch 138/1000\n",
      "121/121 [==============================] - 7s 57ms/step - loss: 0.0633 - accuracy: 0.2754\n",
      "Epoch 139/1000\n",
      "121/121 [==============================] - 7s 57ms/step - loss: 0.0651 - accuracy: 0.2733\n",
      "Epoch 140/1000\n",
      "121/121 [==============================] - 7s 56ms/step - loss: 0.0699 - accuracy: 0.2676\n",
      "Epoch 141/1000\n",
      "121/121 [==============================] - 7s 56ms/step - loss: 0.0629 - accuracy: 0.2825\n",
      "Epoch 142/1000\n",
      "121/121 [==============================] - 7s 57ms/step - loss: 0.0609 - accuracy: 0.2724\n",
      "Epoch 143/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0745 - accuracy: 0.2717\n",
      "Epoch 144/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0635 - accuracy: 0.2721\n",
      "Epoch 145/1000\n",
      "121/121 [==============================] - 7s 56ms/step - loss: 0.0657 - accuracy: 0.2707\n",
      "Epoch 146/1000\n",
      "121/121 [==============================] - 7s 56ms/step - loss: 0.0736 - accuracy: 0.2768\n",
      "Epoch 147/1000\n",
      "121/121 [==============================] - 7s 56ms/step - loss: 0.0574 - accuracy: 0.2859\n",
      "Epoch 148/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0550 - accuracy: 0.2810\n",
      "Epoch 149/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0666 - accuracy: 0.2804\n",
      "Epoch 150/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0676 - accuracy: 0.2760\n",
      "Epoch 151/1000\n",
      "121/121 [==============================] - 7s 54ms/step - loss: 0.0628 - accuracy: 0.2792\n",
      "Epoch 152/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0557 - accuracy: 0.2836\n",
      "Epoch 153/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0649 - accuracy: 0.2864\n",
      "Epoch 154/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0673 - accuracy: 0.2979\n",
      "Epoch 155/1000\n",
      "121/121 [==============================] - 7s 54ms/step - loss: 0.0592 - accuracy: 0.2834\n",
      "Epoch 156/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0620 - accuracy: 0.2910\n",
      "Epoch 157/1000\n",
      "121/121 [==============================] - 8s 63ms/step - loss: 0.0689 - accuracy: 0.2808\n",
      "Epoch 158/1000\n",
      "121/121 [==============================] - 7s 56ms/step - loss: 0.0552 - accuracy: 0.2807\n",
      "Epoch 159/1000\n",
      "121/121 [==============================] - 7s 54ms/step - loss: 0.0622 - accuracy: 0.2791\n",
      "Epoch 160/1000\n",
      "121/121 [==============================] - 7s 54ms/step - loss: 0.0589 - accuracy: 0.2769\n",
      "Epoch 161/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0583 - accuracy: 0.2818\n",
      "Epoch 162/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0880 - accuracy: 0.2686\n",
      "Epoch 163/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0630 - accuracy: 0.2842\n",
      "Epoch 164/1000\n",
      "121/121 [==============================] - 7s 57ms/step - loss: 0.0582 - accuracy: 0.2968\n",
      "Epoch 165/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0540 - accuracy: 0.2941\n",
      "Epoch 166/1000\n",
      "121/121 [==============================] - 7s 56ms/step - loss: 0.0534 - accuracy: 0.3028\n",
      "Epoch 167/1000\n",
      "121/121 [==============================] - 7s 59ms/step - loss: 0.0576 - accuracy: 0.2887\n",
      "Epoch 168/1000\n",
      "121/121 [==============================] - 7s 56ms/step - loss: 0.0602 - accuracy: 0.2963\n",
      "Epoch 169/1000\n",
      "121/121 [==============================] - 20s 166ms/step - loss: 0.0738 - accuracy: 0.2941\n",
      "Epoch 170/1000\n",
      "121/121 [==============================] - 8s 66ms/step - loss: 0.0636 - accuracy: 0.3034\n",
      "Epoch 171/1000\n",
      "121/121 [==============================] - 8s 68ms/step - loss: 0.0542 - accuracy: 0.3019\n",
      "Epoch 172/1000\n",
      "121/121 [==============================] - 7s 56ms/step - loss: 0.0709 - accuracy: 0.3022\n",
      "Epoch 173/1000\n",
      "121/121 [==============================] - 7s 56ms/step - loss: 0.0503 - accuracy: 0.3011\n",
      "Epoch 174/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0510 - accuracy: 0.3005\n",
      "Epoch 175/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0608 - accuracy: 0.2956\n",
      "Epoch 176/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0563 - accuracy: 0.2943\n",
      "Epoch 177/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0509 - accuracy: 0.2914\n",
      "Epoch 178/1000\n",
      "121/121 [==============================] - 9s 75ms/step - loss: 0.0532 - accuracy: 0.2940\n",
      "Epoch 179/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0565 - accuracy: 0.2941\n",
      "Epoch 180/1000\n",
      "121/121 [==============================] - 7s 54ms/step - loss: 0.0502 - accuracy: 0.2918\n",
      "Epoch 181/1000\n",
      "121/121 [==============================] - 7s 54ms/step - loss: 0.0703 - accuracy: 0.2969\n",
      "Epoch 182/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0564 - accuracy: 0.2924\n",
      "Epoch 183/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0521 - accuracy: 0.2860\n",
      "Epoch 184/1000\n",
      "121/121 [==============================] - 7s 54ms/step - loss: 0.0571 - accuracy: 0.3049\n",
      "Epoch 185/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0480 - accuracy: 0.3013\n",
      "Epoch 186/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0568 - accuracy: 0.3130\n",
      "Epoch 187/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0473 - accuracy: 0.3062\n",
      "Epoch 188/1000\n",
      "121/121 [==============================] - 7s 59ms/step - loss: 0.0526 - accuracy: 0.3080\n",
      "Epoch 189/1000\n",
      "121/121 [==============================] - 33s 274ms/step - loss: 0.0613 - accuracy: 0.3113\n",
      "Epoch 190/1000\n",
      "121/121 [==============================] - 9s 71ms/step - loss: 0.0676 - accuracy: 0.2982\n",
      "Epoch 191/1000\n",
      "121/121 [==============================] - 7s 58ms/step - loss: 0.0552 - accuracy: 0.3064\n",
      "Epoch 192/1000\n",
      "121/121 [==============================] - 7s 58ms/step - loss: 0.0491 - accuracy: 0.3087\n",
      "Epoch 193/1000\n",
      "121/121 [==============================] - 7s 56ms/step - loss: 0.0510 - accuracy: 0.3026\n",
      "Epoch 194/1000\n",
      "121/121 [==============================] - 7s 54ms/step - loss: 0.0575 - accuracy: 0.3042\n",
      "Epoch 195/1000\n",
      "121/121 [==============================] - 9s 70ms/step - loss: 0.0483 - accuracy: 0.3065\n",
      "Epoch 196/1000\n",
      "121/121 [==============================] - 7s 61ms/step - loss: 0.0597 - accuracy: 0.3154\n",
      "Epoch 197/1000\n",
      "121/121 [==============================] - 7s 54ms/step - loss: 0.0466 - accuracy: 0.3169\n",
      "Epoch 198/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0546 - accuracy: 0.3155\n",
      "Epoch 199/1000\n",
      "121/121 [==============================] - 7s 54ms/step - loss: 0.0458 - accuracy: 0.3125\n",
      "Epoch 200/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0560 - accuracy: 0.3175\n",
      "Epoch 201/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0901 - accuracy: 0.3197\n",
      "Epoch 202/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0484 - accuracy: 0.3074\n",
      "Epoch 203/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0495 - accuracy: 0.3142\n",
      "Epoch 204/1000\n",
      "121/121 [==============================] - 7s 54ms/step - loss: 0.0455 - accuracy: 0.3192\n",
      "Epoch 205/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0506 - accuracy: 0.3234\n",
      "Epoch 206/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0506 - accuracy: 0.3182\n",
      "Epoch 207/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0502 - accuracy: 0.3161\n",
      "Epoch 208/1000\n",
      "121/121 [==============================] - 7s 56ms/step - loss: 0.0608 - accuracy: 0.3209\n",
      "Epoch 209/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0465 - accuracy: 0.3120\n",
      "Epoch 210/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0485 - accuracy: 0.3202\n",
      "Epoch 211/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0506 - accuracy: 0.3172\n",
      "Epoch 212/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0589 - accuracy: 0.3206\n",
      "Epoch 213/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0479 - accuracy: 0.3244\n",
      "Epoch 214/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0583 - accuracy: 0.3245\n",
      "Epoch 215/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0554 - accuracy: 0.3226\n",
      "Epoch 216/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0495 - accuracy: 0.3319\n",
      "Epoch 217/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0532 - accuracy: 0.3237\n",
      "Epoch 218/1000\n",
      "121/121 [==============================] - 7s 57ms/step - loss: 0.0462 - accuracy: 0.3166\n",
      "Epoch 219/1000\n",
      "121/121 [==============================] - 7s 56ms/step - loss: 0.0517 - accuracy: 0.3146\n",
      "Epoch 220/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0438 - accuracy: 0.3184\n",
      "Epoch 221/1000\n",
      "121/121 [==============================] - 7s 54ms/step - loss: 0.0419 - accuracy: 0.3234\n",
      "Epoch 222/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0536 - accuracy: 0.3209\n",
      "Epoch 223/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0426 - accuracy: 0.3243\n",
      "Epoch 224/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0434 - accuracy: 0.3225\n",
      "Epoch 225/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0578 - accuracy: 0.3272\n",
      "Epoch 226/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0418 - accuracy: 0.3262\n",
      "Epoch 227/1000\n",
      "121/121 [==============================] - 7s 54ms/step - loss: 0.0445 - accuracy: 0.3227\n",
      "Epoch 228/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0429 - accuracy: 0.3374\n",
      "Epoch 229/1000\n",
      "121/121 [==============================] - 7s 57ms/step - loss: 0.0461 - accuracy: 0.3329\n",
      "Epoch 230/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0552 - accuracy: 0.3150\n",
      "Epoch 231/1000\n",
      "121/121 [==============================] - 7s 54ms/step - loss: 0.0429 - accuracy: 0.3216\n",
      "Epoch 232/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0420 - accuracy: 0.3278\n",
      "Epoch 233/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0426 - accuracy: 0.3320\n",
      "Epoch 234/1000\n",
      "121/121 [==============================] - 7s 54ms/step - loss: 0.0398 - accuracy: 0.3411\n",
      "Epoch 235/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0482 - accuracy: 0.3278\n",
      "Epoch 236/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0538 - accuracy: 0.3261\n",
      "Epoch 237/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0505 - accuracy: 0.3206\n",
      "Epoch 238/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0405 - accuracy: 0.3253\n",
      "Epoch 239/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0453 - accuracy: 0.3193\n",
      "Epoch 240/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0428 - accuracy: 0.3344\n",
      "Epoch 241/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0413 - accuracy: 0.3366\n",
      "Epoch 242/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0481 - accuracy: 0.3360\n",
      "Epoch 243/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0404 - accuracy: 0.3338\n",
      "Epoch 244/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0450 - accuracy: 0.3368\n",
      "Epoch 245/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0430 - accuracy: 0.3292\n",
      "Epoch 246/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0357 - accuracy: 0.3440\n",
      "Epoch 247/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0421 - accuracy: 0.3544\n",
      "Epoch 248/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0388 - accuracy: 0.3397\n",
      "Epoch 249/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0464 - accuracy: 0.3341\n",
      "Epoch 250/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0446 - accuracy: 0.3294\n",
      "Epoch 251/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0458 - accuracy: 0.3370\n",
      "Epoch 252/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0452 - accuracy: 0.3397\n",
      "Epoch 253/1000\n",
      "121/121 [==============================] - 7s 56ms/step - loss: 0.0410 - accuracy: 0.3411\n",
      "Epoch 254/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0351 - accuracy: 0.3431\n",
      "Epoch 255/1000\n",
      "121/121 [==============================] - 7s 57ms/step - loss: 0.0453 - accuracy: 0.3488\n",
      "Epoch 256/1000\n",
      "121/121 [==============================] - 7s 54ms/step - loss: 0.0434 - accuracy: 0.3482\n",
      "Epoch 257/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0399 - accuracy: 0.3596\n",
      "Epoch 258/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0386 - accuracy: 0.3479\n",
      "Epoch 259/1000\n",
      "121/121 [==============================] - 7s 54ms/step - loss: 0.0441 - accuracy: 0.3541\n",
      "Epoch 260/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0371 - accuracy: 0.3481\n",
      "Epoch 261/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0335 - accuracy: 0.3398\n",
      "Epoch 262/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0391 - accuracy: 0.3429\n",
      "Epoch 263/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0416 - accuracy: 0.3438\n",
      "Epoch 264/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0389 - accuracy: 0.3492\n",
      "Epoch 265/1000\n",
      "121/121 [==============================] - 7s 54ms/step - loss: 0.0418 - accuracy: 0.3577\n",
      "Epoch 266/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0461 - accuracy: 0.3592\n",
      "Epoch 267/1000\n",
      "121/121 [==============================] - 7s 54ms/step - loss: 0.0375 - accuracy: 0.3588\n",
      "Epoch 268/1000\n",
      "121/121 [==============================] - 7s 54ms/step - loss: 0.0384 - accuracy: 0.3518\n",
      "Epoch 269/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0388 - accuracy: 0.3597\n",
      "Epoch 270/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0450 - accuracy: 0.3650\n",
      "Epoch 271/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0352 - accuracy: 0.3685\n",
      "Epoch 272/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0502 - accuracy: 0.3574\n",
      "Epoch 273/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0367 - accuracy: 0.3609\n",
      "Epoch 274/1000\n",
      "121/121 [==============================] - 7s 56ms/step - loss: 0.0342 - accuracy: 0.3697\n",
      "Epoch 275/1000\n",
      "121/121 [==============================] - 7s 57ms/step - loss: 0.0357 - accuracy: 0.3567\n",
      "Epoch 276/1000\n",
      "121/121 [==============================] - 7s 60ms/step - loss: 0.0378 - accuracy: 0.3516\n",
      "Epoch 277/1000\n",
      "121/121 [==============================] - 7s 56ms/step - loss: 0.0340 - accuracy: 0.3568\n",
      "Epoch 278/1000\n",
      "121/121 [==============================] - 7s 58ms/step - loss: 0.0362 - accuracy: 0.3702\n",
      "Epoch 279/1000\n",
      "121/121 [==============================] - 7s 60ms/step - loss: 0.0379 - accuracy: 0.3674\n",
      "Epoch 280/1000\n",
      "121/121 [==============================] - 7s 58ms/step - loss: 0.0333 - accuracy: 0.3553\n",
      "Epoch 281/1000\n",
      "121/121 [==============================] - 7s 58ms/step - loss: 0.0529 - accuracy: 0.3758\n",
      "Epoch 282/1000\n",
      "121/121 [==============================] - 7s 61ms/step - loss: 0.0468 - accuracy: 0.3682\n",
      "Epoch 283/1000\n",
      "121/121 [==============================] - 7s 56ms/step - loss: 0.0369 - accuracy: 0.3697\n",
      "Epoch 284/1000\n",
      "121/121 [==============================] - 7s 58ms/step - loss: 0.0331 - accuracy: 0.3556\n",
      "Epoch 285/1000\n",
      "121/121 [==============================] - 7s 56ms/step - loss: 0.0313 - accuracy: 0.3711\n",
      "Epoch 286/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0365 - accuracy: 0.3598\n",
      "Epoch 287/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0320 - accuracy: 0.3622\n",
      "Epoch 288/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0279 - accuracy: 0.3671\n",
      "Epoch 289/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0298 - accuracy: 0.3647\n",
      "Epoch 290/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0357 - accuracy: 0.3691\n",
      "Epoch 291/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0379 - accuracy: 0.3668\n",
      "Epoch 292/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0318 - accuracy: 0.3726\n",
      "Epoch 293/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0334 - accuracy: 0.3667\n",
      "Epoch 294/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0444 - accuracy: 0.3615\n",
      "Epoch 295/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0307 - accuracy: 0.3643\n",
      "Epoch 296/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0439 - accuracy: 0.3789\n",
      "Epoch 297/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0302 - accuracy: 0.3723\n",
      "Epoch 298/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0463 - accuracy: 0.3766\n",
      "Epoch 299/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0296 - accuracy: 0.3810\n",
      "Epoch 300/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0424 - accuracy: 0.3729\n",
      "Epoch 301/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0306 - accuracy: 0.3668\n",
      "Epoch 302/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0315 - accuracy: 0.3651\n",
      "Epoch 303/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0326 - accuracy: 0.3759\n",
      "Epoch 304/1000\n",
      "121/121 [==============================] - 7s 56ms/step - loss: 0.0428 - accuracy: 0.3559\n",
      "Epoch 305/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0306 - accuracy: 0.3593\n",
      "Epoch 306/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0297 - accuracy: 0.3684\n",
      "Epoch 307/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0313 - accuracy: 0.3667\n",
      "Epoch 308/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0342 - accuracy: 0.3678\n",
      "Epoch 309/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0262 - accuracy: 0.3709\n",
      "Epoch 310/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0416 - accuracy: 0.3552\n",
      "Epoch 311/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0312 - accuracy: 0.3639\n",
      "Epoch 312/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0294 - accuracy: 0.3692\n",
      "Epoch 313/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0379 - accuracy: 0.3773\n",
      "Epoch 314/1000\n",
      "121/121 [==============================] - 87s 723ms/step - loss: 0.0282 - accuracy: 0.3798\n",
      "Epoch 315/1000\n",
      "121/121 [==============================] - 8s 65ms/step - loss: 0.0299 - accuracy: 0.3804\n",
      "Epoch 316/1000\n",
      "121/121 [==============================] - 8s 67ms/step - loss: 0.0363 - accuracy: 0.3679\n",
      "Epoch 317/1000\n",
      "121/121 [==============================] - 7s 56ms/step - loss: 0.0273 - accuracy: 0.3806\n",
      "Epoch 318/1000\n",
      "121/121 [==============================] - 7s 56ms/step - loss: 0.0586 - accuracy: 0.3718\n",
      "Epoch 319/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0362 - accuracy: 0.3768\n",
      "Epoch 320/1000\n",
      "121/121 [==============================] - 9s 75ms/step - loss: 0.0302 - accuracy: 0.3799\n",
      "Epoch 321/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0336 - accuracy: 0.3749\n",
      "Epoch 322/1000\n",
      "121/121 [==============================] - 7s 56ms/step - loss: 0.0330 - accuracy: 0.3669\n",
      "Epoch 323/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0268 - accuracy: 0.3675\n",
      "Epoch 324/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0290 - accuracy: 0.3719\n",
      "Epoch 325/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0251 - accuracy: 0.3750\n",
      "Epoch 326/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0303 - accuracy: 0.3707\n",
      "Epoch 327/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0259 - accuracy: 0.3756\n",
      "Epoch 328/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0297 - accuracy: 0.3778\n",
      "Epoch 329/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0295 - accuracy: 0.3992\n",
      "Epoch 330/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0378 - accuracy: 0.4016\n",
      "Epoch 331/1000\n",
      "121/121 [==============================] - 7s 56ms/step - loss: 0.0362 - accuracy: 0.3889\n",
      "Epoch 332/1000\n",
      "121/121 [==============================] - 7s 56ms/step - loss: 0.0298 - accuracy: 0.3896\n",
      "Epoch 333/1000\n",
      "121/121 [==============================] - 7s 58ms/step - loss: 0.0295 - accuracy: 0.3860\n",
      "Epoch 334/1000\n",
      "121/121 [==============================] - 46s 383ms/step - loss: 0.0328 - accuracy: 0.3928\n",
      "Epoch 335/1000\n",
      "121/121 [==============================] - 9s 72ms/step - loss: 0.0357 - accuracy: 0.3853\n",
      "Epoch 336/1000\n",
      "121/121 [==============================] - 7s 57ms/step - loss: 0.0247 - accuracy: 0.3892\n",
      "Epoch 337/1000\n",
      "121/121 [==============================] - 7s 57ms/step - loss: 0.0260 - accuracy: 0.3925\n",
      "Epoch 338/1000\n",
      "121/121 [==============================] - 7s 58ms/step - loss: 0.0381 - accuracy: 0.3842\n",
      "Epoch 339/1000\n",
      "121/121 [==============================] - 7s 60ms/step - loss: 0.0280 - accuracy: 0.3925\n",
      "Epoch 340/1000\n",
      "121/121 [==============================] - 10s 81ms/step - loss: 0.0361 - accuracy: 0.3871\n",
      "Epoch 341/1000\n",
      "121/121 [==============================] - 7s 58ms/step - loss: 0.0241 - accuracy: 0.3878\n",
      "Epoch 342/1000\n",
      "121/121 [==============================] - 7s 56ms/step - loss: 0.0228 - accuracy: 0.3979\n",
      "Epoch 343/1000\n",
      "121/121 [==============================] - 7s 56ms/step - loss: 0.0272 - accuracy: 0.3898\n",
      "Epoch 344/1000\n",
      "121/121 [==============================] - 7s 56ms/step - loss: 0.0284 - accuracy: 0.3896\n",
      "Epoch 345/1000\n",
      "121/121 [==============================] - 7s 56ms/step - loss: 0.0326 - accuracy: 0.3879\n",
      "Epoch 346/1000\n",
      "121/121 [==============================] - 7s 56ms/step - loss: 0.0248 - accuracy: 0.3857\n",
      "Epoch 347/1000\n",
      "121/121 [==============================] - 7s 56ms/step - loss: 0.0267 - accuracy: 0.3963\n",
      "Epoch 348/1000\n",
      "121/121 [==============================] - 7s 56ms/step - loss: 0.0257 - accuracy: 0.3968\n",
      "Epoch 349/1000\n",
      "121/121 [==============================] - 7s 58ms/step - loss: 0.0236 - accuracy: 0.3929\n",
      "Epoch 350/1000\n",
      "121/121 [==============================] - 7s 59ms/step - loss: 0.0315 - accuracy: 0.3986\n",
      "Epoch 351/1000\n",
      "121/121 [==============================] - 7s 56ms/step - loss: 0.0246 - accuracy: 0.4025\n",
      "Epoch 352/1000\n",
      "121/121 [==============================] - 7s 54ms/step - loss: 0.0304 - accuracy: 0.4040\n",
      "Epoch 353/1000\n",
      "121/121 [==============================] - 7s 54ms/step - loss: 0.0354 - accuracy: 0.3964\n",
      "Epoch 354/1000\n",
      "121/121 [==============================] - 6s 54ms/step - loss: 0.0381 - accuracy: 0.3826\n",
      "Epoch 355/1000\n",
      "121/121 [==============================] - 6s 54ms/step - loss: 0.0402 - accuracy: 0.3861\n",
      "Epoch 356/1000\n",
      "121/121 [==============================] - 7s 54ms/step - loss: 0.0275 - accuracy: 0.3869\n",
      "Epoch 357/1000\n",
      "121/121 [==============================] - 6s 54ms/step - loss: 0.0346 - accuracy: 0.3954\n",
      "Epoch 358/1000\n",
      "121/121 [==============================] - 7s 54ms/step - loss: 0.0288 - accuracy: 0.3971\n",
      "Epoch 359/1000\n",
      "121/121 [==============================] - 7s 54ms/step - loss: 0.0218 - accuracy: 0.3994\n",
      "Epoch 360/1000\n",
      "121/121 [==============================] - 6s 54ms/step - loss: 0.0216 - accuracy: 0.4100\n",
      "Epoch 361/1000\n",
      "121/121 [==============================] - 7s 54ms/step - loss: 0.0226 - accuracy: 0.4156\n",
      "Epoch 362/1000\n",
      "121/121 [==============================] - 6s 53ms/step - loss: 0.0328 - accuracy: 0.4037\n",
      "Epoch 363/1000\n",
      "121/121 [==============================] - 7s 54ms/step - loss: 0.0268 - accuracy: 0.3878\n",
      "Epoch 364/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0243 - accuracy: 0.4058\n",
      "Epoch 365/1000\n",
      "121/121 [==============================] - 6s 53ms/step - loss: 0.0251 - accuracy: 0.4143\n",
      "Epoch 366/1000\n",
      "121/121 [==============================] - 7s 54ms/step - loss: 0.0265 - accuracy: 0.4078\n",
      "Epoch 367/1000\n",
      "121/121 [==============================] - 7s 54ms/step - loss: 0.0291 - accuracy: 0.4019\n",
      "Epoch 368/1000\n",
      "121/121 [==============================] - 7s 54ms/step - loss: 0.0309 - accuracy: 0.4058\n",
      "Epoch 369/1000\n",
      "121/121 [==============================] - 7s 54ms/step - loss: 0.0288 - accuracy: 0.3963\n",
      "Epoch 370/1000\n",
      "121/121 [==============================] - 7s 54ms/step - loss: 0.0313 - accuracy: 0.3940\n",
      "Epoch 371/1000\n",
      "121/121 [==============================] - 7s 54ms/step - loss: 0.0197 - accuracy: 0.3991\n",
      "Epoch 372/1000\n",
      "121/121 [==============================] - 7s 56ms/step - loss: 0.0193 - accuracy: 0.4243\n",
      "Epoch 373/1000\n",
      "121/121 [==============================] - 8s 65ms/step - loss: 0.0178 - accuracy: 0.4215\n",
      "Epoch 374/1000\n",
      "121/121 [==============================] - 7s 57ms/step - loss: 0.0221 - accuracy: 0.4194\n",
      "Epoch 375/1000\n",
      "121/121 [==============================] - 7s 57ms/step - loss: 0.0261 - accuracy: 0.4224\n",
      "Epoch 376/1000\n",
      "121/121 [==============================] - 8s 65ms/step - loss: 0.0290 - accuracy: 0.4088\n",
      "Epoch 377/1000\n",
      "121/121 [==============================] - 7s 59ms/step - loss: 0.0245 - accuracy: 0.4009\n",
      "Epoch 378/1000\n",
      "121/121 [==============================] - 7s 60ms/step - loss: 0.0355 - accuracy: 0.4000\n",
      "Epoch 379/1000\n",
      "121/121 [==============================] - 7s 60ms/step - loss: 0.0356 - accuracy: 0.4102\n",
      "Epoch 380/1000\n",
      "121/121 [==============================] - 7s 56ms/step - loss: 0.0335 - accuracy: 0.4041\n",
      "Epoch 381/1000\n",
      "121/121 [==============================] - 7s 56ms/step - loss: 0.0333 - accuracy: 0.4026\n",
      "Epoch 382/1000\n",
      "121/121 [==============================] - 7s 56ms/step - loss: 0.0312 - accuracy: 0.4114\n",
      "Epoch 383/1000\n",
      "121/121 [==============================] - 7s 56ms/step - loss: 0.0196 - accuracy: 0.4201\n",
      "Epoch 384/1000\n",
      "121/121 [==============================] - 7s 56ms/step - loss: 0.0167 - accuracy: 0.4277\n",
      "Epoch 385/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0206 - accuracy: 0.4284\n",
      "Epoch 386/1000\n",
      "121/121 [==============================] - 7s 56ms/step - loss: 0.0260 - accuracy: 0.4134\n",
      "Epoch 387/1000\n",
      "121/121 [==============================] - 7s 56ms/step - loss: 0.0228 - accuracy: 0.4251\n",
      "Epoch 388/1000\n",
      "121/121 [==============================] - 7s 56ms/step - loss: 0.0344 - accuracy: 0.4216\n",
      "Epoch 389/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0201 - accuracy: 0.4168\n",
      "Epoch 390/1000\n",
      "121/121 [==============================] - 7s 56ms/step - loss: 0.0209 - accuracy: 0.4232\n",
      "Epoch 391/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0344 - accuracy: 0.4108\n",
      "Epoch 392/1000\n",
      "121/121 [==============================] - 7s 56ms/step - loss: 0.0301 - accuracy: 0.4137\n",
      "Epoch 393/1000\n",
      "121/121 [==============================] - 7s 56ms/step - loss: 0.0239 - accuracy: 0.4116\n",
      "Epoch 394/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0212 - accuracy: 0.4217\n",
      "Epoch 395/1000\n",
      "121/121 [==============================] - 7s 56ms/step - loss: 0.0241 - accuracy: 0.4231\n",
      "Epoch 396/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0273 - accuracy: 0.4182\n",
      "Epoch 397/1000\n",
      "121/121 [==============================] - 7s 56ms/step - loss: 0.0229 - accuracy: 0.4320\n",
      "Epoch 398/1000\n",
      "121/121 [==============================] - 7s 56ms/step - loss: 0.0228 - accuracy: 0.4266\n",
      "Epoch 399/1000\n",
      "121/121 [==============================] - 7s 56ms/step - loss: 0.0304 - accuracy: 0.4276\n",
      "Epoch 400/1000\n",
      "121/121 [==============================] - 7s 59ms/step - loss: 0.0214 - accuracy: 0.4172\n",
      "Epoch 401/1000\n",
      "121/121 [==============================] - 79s 659ms/step - loss: 0.0175 - accuracy: 0.4106\n",
      "Epoch 402/1000\n",
      "121/121 [==============================] - 9s 76ms/step - loss: 0.0272 - accuracy: 0.4084\n",
      "Epoch 403/1000\n",
      "121/121 [==============================] - 7s 58ms/step - loss: 0.0256 - accuracy: 0.4173\n",
      "Epoch 404/1000\n",
      "121/121 [==============================] - 7s 57ms/step - loss: 0.0280 - accuracy: 0.4043\n",
      "Epoch 405/1000\n",
      "121/121 [==============================] - 7s 56ms/step - loss: 0.0290 - accuracy: 0.4100\n",
      "Epoch 406/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0226 - accuracy: 0.4186\n",
      "Epoch 407/1000\n",
      "121/121 [==============================] - 8s 69ms/step - loss: 0.0181 - accuracy: 0.4260\n",
      "Epoch 408/1000\n",
      "121/121 [==============================] - 8s 63ms/step - loss: 0.0161 - accuracy: 0.4242\n",
      "Epoch 409/1000\n",
      "121/121 [==============================] - 7s 56ms/step - loss: 0.0204 - accuracy: 0.4193\n",
      "Epoch 410/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0200 - accuracy: 0.4196\n",
      "Epoch 411/1000\n",
      "121/121 [==============================] - 7s 57ms/step - loss: 0.0207 - accuracy: 0.4240\n",
      "Epoch 412/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0373 - accuracy: 0.4256\n",
      "Epoch 413/1000\n",
      "121/121 [==============================] - 7s 56ms/step - loss: 0.0283 - accuracy: 0.4258\n",
      "Epoch 414/1000\n",
      "121/121 [==============================] - 7s 56ms/step - loss: 0.0219 - accuracy: 0.4131\n",
      "Epoch 415/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0198 - accuracy: 0.4206\n",
      "Epoch 416/1000\n",
      "121/121 [==============================] - 7s 56ms/step - loss: 0.0176 - accuracy: 0.4225\n",
      "Epoch 417/1000\n",
      "121/121 [==============================] - 7s 57ms/step - loss: 0.0191 - accuracy: 0.4315\n",
      "Epoch 418/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0281 - accuracy: 0.4293\n",
      "Epoch 419/1000\n",
      "121/121 [==============================] - 7s 60ms/step - loss: 0.0291 - accuracy: 0.4146\n",
      "Epoch 420/1000\n",
      "121/121 [==============================] - 170s 1s/step - loss: 0.0268 - accuracy: 0.4161\n",
      "Epoch 421/1000\n",
      "121/121 [==============================] - 160s 1s/step - loss: 0.0243 - accuracy: 0.4296\n",
      "Epoch 422/1000\n",
      "121/121 [==============================] - 8s 67ms/step - loss: 0.0199 - accuracy: 0.4274\n",
      "Epoch 423/1000\n",
      "121/121 [==============================] - 8s 67ms/step - loss: 0.0209 - accuracy: 0.4313\n",
      "Epoch 424/1000\n",
      "121/121 [==============================] - 7s 57ms/step - loss: 0.0191 - accuracy: 0.4321\n",
      "Epoch 425/1000\n",
      "121/121 [==============================] - 7s 58ms/step - loss: 0.0224 - accuracy: 0.4297\n",
      "Epoch 426/1000\n",
      "121/121 [==============================] - 7s 57ms/step - loss: 0.0323 - accuracy: 0.4237\n",
      "Epoch 427/1000\n",
      "121/121 [==============================] - 7s 57ms/step - loss: 0.0276 - accuracy: 0.4248\n",
      "Epoch 428/1000\n",
      "121/121 [==============================] - 9s 77ms/step - loss: 0.0314 - accuracy: 0.4229\n",
      "Epoch 429/1000\n",
      "121/121 [==============================] - 7s 57ms/step - loss: 0.0215 - accuracy: 0.4201\n",
      "Epoch 430/1000\n",
      "121/121 [==============================] - 7s 61ms/step - loss: 0.0192 - accuracy: 0.4207\n",
      "Epoch 431/1000\n",
      "121/121 [==============================] - 7s 57ms/step - loss: 0.0169 - accuracy: 0.4296\n",
      "Epoch 432/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0213 - accuracy: 0.4305\n",
      "Epoch 433/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0219 - accuracy: 0.4182\n",
      "Epoch 434/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0286 - accuracy: 0.4136\n",
      "Epoch 435/1000\n",
      "121/121 [==============================] - 7s 58ms/step - loss: 0.0240 - accuracy: 0.4192\n",
      "Epoch 436/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0173 - accuracy: 0.4308\n",
      "Epoch 437/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0150 - accuracy: 0.4323\n",
      "Epoch 438/1000\n",
      "121/121 [==============================] - 7s 56ms/step - loss: 0.0144 - accuracy: 0.4395\n",
      "Epoch 439/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0171 - accuracy: 0.4487\n",
      "Epoch 440/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0220 - accuracy: 0.4381\n",
      "Epoch 441/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0203 - accuracy: 0.4384\n",
      "Epoch 442/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0192 - accuracy: 0.4442\n",
      "Epoch 443/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0348 - accuracy: 0.4381\n",
      "Epoch 444/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0203 - accuracy: 0.4396\n",
      "Epoch 445/1000\n",
      "121/121 [==============================] - 7s 54ms/step - loss: 0.0232 - accuracy: 0.4342\n",
      "Epoch 446/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0210 - accuracy: 0.4259\n",
      "Epoch 447/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0247 - accuracy: 0.4396\n",
      "Epoch 448/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0266 - accuracy: 0.4373\n",
      "Epoch 449/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0510 - accuracy: 0.4361\n",
      "Epoch 450/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0174 - accuracy: 0.4338\n",
      "Epoch 451/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0202 - accuracy: 0.4438\n",
      "Epoch 452/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0182 - accuracy: 0.4454\n",
      "Epoch 453/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0183 - accuracy: 0.4374\n",
      "Epoch 454/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0165 - accuracy: 0.4288\n",
      "Epoch 455/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0241 - accuracy: 0.4333\n",
      "Epoch 456/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0248 - accuracy: 0.4201\n",
      "Epoch 457/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0182 - accuracy: 0.4366\n",
      "Epoch 458/1000\n",
      "121/121 [==============================] - 7s 56ms/step - loss: 0.0195 - accuracy: 0.4369\n",
      "Epoch 459/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0156 - accuracy: 0.4352\n",
      "Epoch 460/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0208 - accuracy: 0.4402\n",
      "Epoch 461/1000\n",
      "121/121 [==============================] - 7s 54ms/step - loss: 0.0206 - accuracy: 0.4407\n",
      "Epoch 462/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0335 - accuracy: 0.4386\n",
      "Epoch 463/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0197 - accuracy: 0.4345\n",
      "Epoch 464/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0231 - accuracy: 0.4412\n",
      "Epoch 465/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0235 - accuracy: 0.4424\n",
      "Epoch 466/1000\n",
      "121/121 [==============================] - 7s 57ms/step - loss: 0.0240 - accuracy: 0.4404\n",
      "Epoch 467/1000\n",
      "121/121 [==============================] - 7s 56ms/step - loss: 0.0356 - accuracy: 0.4377\n",
      "Epoch 468/1000\n",
      "121/121 [==============================] - 7s 56ms/step - loss: 0.0175 - accuracy: 0.4367\n",
      "Epoch 469/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0301 - accuracy: 0.4342\n",
      "Epoch 470/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0281 - accuracy: 0.4339\n",
      "Epoch 471/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0153 - accuracy: 0.4520\n",
      "Epoch 472/1000\n",
      "121/121 [==============================] - 7s 56ms/step - loss: 0.0223 - accuracy: 0.4438\n",
      "Epoch 473/1000\n",
      "121/121 [==============================] - 7s 56ms/step - loss: 0.0325 - accuracy: 0.4485\n",
      "Epoch 474/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0172 - accuracy: 0.4493\n",
      "Epoch 475/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0167 - accuracy: 0.4551\n",
      "Epoch 476/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0198 - accuracy: 0.4456\n",
      "Epoch 477/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0199 - accuracy: 0.4504\n",
      "Epoch 478/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0281 - accuracy: 0.4483\n",
      "Epoch 479/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.1036 - accuracy: 0.4457\n",
      "Epoch 480/1000\n",
      "121/121 [==============================] - 7s 56ms/step - loss: 0.0205 - accuracy: 0.4415\n",
      "Epoch 481/1000\n",
      "121/121 [==============================] - 7s 57ms/step - loss: 0.0173 - accuracy: 0.4475\n",
      "Epoch 482/1000\n",
      "121/121 [==============================] - 7s 56ms/step - loss: 0.0214 - accuracy: 0.4406\n",
      "Epoch 483/1000\n",
      "121/121 [==============================] - 7s 56ms/step - loss: 0.0187 - accuracy: 0.4461\n",
      "Epoch 484/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0183 - accuracy: 0.4439\n",
      "Epoch 485/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0190 - accuracy: 0.4361\n",
      "Epoch 486/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0216 - accuracy: 0.4469\n",
      "Epoch 487/1000\n",
      "121/121 [==============================] - 7s 56ms/step - loss: 0.0281 - accuracy: 0.4408\n",
      "Epoch 488/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0207 - accuracy: 0.4382\n",
      "Epoch 489/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0136 - accuracy: 0.4414\n",
      "Epoch 490/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0258 - accuracy: 0.4466\n",
      "Epoch 491/1000\n",
      "121/121 [==============================] - 7s 56ms/step - loss: 0.0264 - accuracy: 0.4332\n",
      "Epoch 492/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0158 - accuracy: 0.4458\n",
      "Epoch 493/1000\n",
      "121/121 [==============================] - 7s 56ms/step - loss: 0.0155 - accuracy: 0.4448\n",
      "Epoch 494/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0206 - accuracy: 0.4426\n",
      "Epoch 495/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0295 - accuracy: 0.4331\n",
      "Epoch 496/1000\n",
      "121/121 [==============================] - 7s 56ms/step - loss: 0.0183 - accuracy: 0.4425\n",
      "Epoch 497/1000\n",
      "121/121 [==============================] - 7s 56ms/step - loss: 0.0132 - accuracy: 0.4508\n",
      "Epoch 498/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0205 - accuracy: 0.4584\n",
      "Epoch 499/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0317 - accuracy: 0.4561\n",
      "Epoch 500/1000\n",
      "121/121 [==============================] - 7s 56ms/step - loss: 0.0174 - accuracy: 0.4556\n",
      "Epoch 501/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0177 - accuracy: 0.4486\n",
      "Epoch 502/1000\n",
      "121/121 [==============================] - 7s 56ms/step - loss: 0.0195 - accuracy: 0.4517\n",
      "Epoch 503/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0216 - accuracy: 0.4452\n",
      "Epoch 504/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0193 - accuracy: 0.4512\n",
      "Epoch 505/1000\n",
      "121/121 [==============================] - 7s 56ms/step - loss: 0.0185 - accuracy: 0.4599\n",
      "Epoch 506/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0248 - accuracy: 0.4617\n",
      "Epoch 507/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0209 - accuracy: 0.4546\n",
      "Epoch 508/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0170 - accuracy: 0.4533\n",
      "Epoch 509/1000\n",
      "121/121 [==============================] - 7s 56ms/step - loss: 0.0198 - accuracy: 0.4633\n",
      "Epoch 510/1000\n",
      "121/121 [==============================] - 7s 56ms/step - loss: 0.0207 - accuracy: 0.4636\n",
      "Epoch 511/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0145 - accuracy: 0.4710\n",
      "Epoch 512/1000\n",
      "121/121 [==============================] - 7s 58ms/step - loss: 0.0141 - accuracy: 0.4689\n",
      "Epoch 513/1000\n",
      "121/121 [==============================] - 8s 67ms/step - loss: 0.0133 - accuracy: 0.4696\n",
      "Epoch 514/1000\n",
      "121/121 [==============================] - 10s 85ms/step - loss: 0.0129 - accuracy: 0.4707\n",
      "Epoch 515/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0189 - accuracy: 0.4711\n",
      "Epoch 516/1000\n",
      "121/121 [==============================] - 7s 54ms/step - loss: 0.0203 - accuracy: 0.4666\n",
      "Epoch 517/1000\n",
      "121/121 [==============================] - 6s 53ms/step - loss: 0.0157 - accuracy: 0.4678\n",
      "Epoch 518/1000\n",
      "121/121 [==============================] - 6s 53ms/step - loss: 0.0235 - accuracy: 0.4583\n",
      "Epoch 519/1000\n",
      "121/121 [==============================] - 33s 276ms/step - loss: 0.0184 - accuracy: 0.4549\n",
      "Epoch 520/1000\n",
      "121/121 [==============================] - 7s 61ms/step - loss: 0.0172 - accuracy: 0.4560\n",
      "Epoch 521/1000\n",
      "121/121 [==============================] - 7s 59ms/step - loss: 0.0170 - accuracy: 0.4723\n",
      "Epoch 522/1000\n",
      "121/121 [==============================] - 6s 53ms/step - loss: 0.0383 - accuracy: 0.4452\n",
      "Epoch 523/1000\n",
      "121/121 [==============================] - 6s 53ms/step - loss: 0.0308 - accuracy: 0.4577\n",
      "Epoch 524/1000\n",
      "121/121 [==============================] - 6s 53ms/step - loss: 0.0234 - accuracy: 0.4496\n",
      "Epoch 525/1000\n",
      "121/121 [==============================] - 6s 53ms/step - loss: 0.0249 - accuracy: 0.4538\n",
      "Epoch 526/1000\n",
      "121/121 [==============================] - 7s 60ms/step - loss: 0.0227 - accuracy: 0.4471\n",
      "Epoch 527/1000\n",
      "121/121 [==============================] - 6s 53ms/step - loss: 0.0208 - accuracy: 0.4545\n",
      "Epoch 528/1000\n",
      "121/121 [==============================] - 7s 54ms/step - loss: 0.0144 - accuracy: 0.4623\n",
      "Epoch 529/1000\n",
      "121/121 [==============================] - 6s 53ms/step - loss: 0.0174 - accuracy: 0.4661\n",
      "Epoch 530/1000\n",
      "121/121 [==============================] - 6s 53ms/step - loss: 0.0178 - accuracy: 0.4576\n",
      "Epoch 531/1000\n",
      "121/121 [==============================] - 6s 53ms/step - loss: 0.0189 - accuracy: 0.4548\n",
      "Epoch 532/1000\n",
      "121/121 [==============================] - 6s 53ms/step - loss: 0.0249 - accuracy: 0.4623\n",
      "Epoch 533/1000\n",
      "121/121 [==============================] - 40s 332ms/step - loss: 0.0221 - accuracy: 0.4538\n",
      "Epoch 534/1000\n",
      "121/121 [==============================] - 8s 63ms/step - loss: 0.0186 - accuracy: 0.4621\n",
      "Epoch 535/1000\n",
      "121/121 [==============================] - 8s 66ms/step - loss: 0.0170 - accuracy: 0.4719\n",
      "Epoch 536/1000\n",
      "121/121 [==============================] - 7s 56ms/step - loss: 0.0172 - accuracy: 0.4619\n",
      "Epoch 537/1000\n",
      "121/121 [==============================] - 7s 56ms/step - loss: 0.0245 - accuracy: 0.4629\n",
      "Epoch 538/1000\n",
      "121/121 [==============================] - 7s 56ms/step - loss: 0.0164 - accuracy: 0.4562\n",
      "Epoch 539/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0311 - accuracy: 0.4505\n",
      "Epoch 540/1000\n",
      "121/121 [==============================] - 9s 74ms/step - loss: 0.0220 - accuracy: 0.4558\n",
      "Epoch 541/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0146 - accuracy: 0.4614\n",
      "Epoch 542/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0275 - accuracy: 0.4678\n",
      "Epoch 543/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0173 - accuracy: 0.4812\n",
      "Epoch 544/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0148 - accuracy: 0.4676\n",
      "Epoch 545/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0135 - accuracy: 0.4606\n",
      "Epoch 546/1000\n",
      "121/121 [==============================] - 7s 56ms/step - loss: 0.0146 - accuracy: 0.4605\n",
      "Epoch 547/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0225 - accuracy: 0.4689\n",
      "Epoch 548/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0161 - accuracy: 0.4729\n",
      "Epoch 549/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0208 - accuracy: 0.4774\n",
      "Epoch 550/1000\n",
      "121/121 [==============================] - 7s 56ms/step - loss: 0.0206 - accuracy: 0.4678\n",
      "Epoch 551/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0217 - accuracy: 0.4732\n",
      "Epoch 552/1000\n",
      "121/121 [==============================] - 7s 56ms/step - loss: 0.0171 - accuracy: 0.4767\n",
      "Epoch 553/1000\n",
      "121/121 [==============================] - 7s 56ms/step - loss: 0.0193 - accuracy: 0.4574\n",
      "Epoch 554/1000\n",
      "121/121 [==============================] - 7s 56ms/step - loss: 0.0167 - accuracy: 0.4580\n",
      "Epoch 555/1000\n",
      "121/121 [==============================] - 7s 56ms/step - loss: 0.0169 - accuracy: 0.4578\n",
      "Epoch 556/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0205 - accuracy: 0.4659\n",
      "Epoch 557/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0191 - accuracy: 0.4658\n",
      "Epoch 558/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0192 - accuracy: 0.4625\n",
      "Epoch 559/1000\n",
      "121/121 [==============================] - 7s 56ms/step - loss: 0.0233 - accuracy: 0.4676\n",
      "Epoch 560/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0193 - accuracy: 0.4715\n",
      "Epoch 561/1000\n",
      "121/121 [==============================] - 7s 56ms/step - loss: 0.0185 - accuracy: 0.4656\n",
      "Epoch 562/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0148 - accuracy: 0.4663\n",
      "Epoch 563/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0182 - accuracy: 0.4769\n",
      "Epoch 564/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0183 - accuracy: 0.4738\n",
      "Epoch 565/1000\n",
      "121/121 [==============================] - 7s 56ms/step - loss: 0.0183 - accuracy: 0.4763\n",
      "Epoch 566/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0185 - accuracy: 0.4732\n",
      "Epoch 567/1000\n",
      "121/121 [==============================] - 7s 56ms/step - loss: 0.0359 - accuracy: 0.4698\n",
      "Epoch 568/1000\n",
      "121/121 [==============================] - 7s 56ms/step - loss: 0.0155 - accuracy: 0.4640\n",
      "Epoch 569/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0303 - accuracy: 0.4745\n",
      "Epoch 570/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0174 - accuracy: 0.4758\n",
      "Epoch 571/1000\n",
      "121/121 [==============================] - 7s 56ms/step - loss: 0.0141 - accuracy: 0.4771\n",
      "Epoch 572/1000\n",
      "121/121 [==============================] - 7s 56ms/step - loss: 0.0155 - accuracy: 0.4701\n",
      "Epoch 573/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0225 - accuracy: 0.4802\n",
      "Epoch 574/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0279 - accuracy: 0.4753\n",
      "Epoch 575/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0159 - accuracy: 0.4713\n",
      "Epoch 576/1000\n",
      "121/121 [==============================] - 7s 56ms/step - loss: 0.0125 - accuracy: 0.4761\n",
      "Epoch 577/1000\n",
      "121/121 [==============================] - 7s 58ms/step - loss: 0.0165 - accuracy: 0.4631\n",
      "Epoch 578/1000\n",
      "121/121 [==============================] - 7s 56ms/step - loss: 0.0167 - accuracy: 0.4703\n",
      "Epoch 579/1000\n",
      "121/121 [==============================] - 7s 56ms/step - loss: 0.0190 - accuracy: 0.4809\n",
      "Epoch 580/1000\n",
      "121/121 [==============================] - 7s 59ms/step - loss: 0.0249 - accuracy: 0.4740\n",
      "Epoch 581/1000\n",
      "121/121 [==============================] - 7s 56ms/step - loss: 0.0186 - accuracy: 0.4735\n",
      "Epoch 582/1000\n",
      "121/121 [==============================] - 7s 56ms/step - loss: 0.0168 - accuracy: 0.4781\n",
      "Epoch 583/1000\n",
      "121/121 [==============================] - 7s 56ms/step - loss: 0.0116 - accuracy: 0.4771\n",
      "Epoch 584/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0171 - accuracy: 0.4892\n",
      "Epoch 585/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0512 - accuracy: 0.4831\n",
      "Epoch 586/1000\n",
      "121/121 [==============================] - 7s 56ms/step - loss: 0.0151 - accuracy: 0.4846\n",
      "Epoch 587/1000\n",
      "121/121 [==============================] - 7s 56ms/step - loss: 0.0136 - accuracy: 0.4840\n",
      "Epoch 588/1000\n",
      "121/121 [==============================] - 7s 57ms/step - loss: 0.0200 - accuracy: 0.4829\n",
      "Epoch 589/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0225 - accuracy: 0.4839\n",
      "Epoch 590/1000\n",
      "121/121 [==============================] - 7s 56ms/step - loss: 0.0198 - accuracy: 0.4784\n",
      "Epoch 591/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0167 - accuracy: 0.4823\n",
      "Epoch 592/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0158 - accuracy: 0.4827\n",
      "Epoch 593/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0219 - accuracy: 0.4782\n",
      "Epoch 594/1000\n",
      "121/121 [==============================] - 7s 56ms/step - loss: 0.0189 - accuracy: 0.4737\n",
      "Epoch 595/1000\n",
      "121/121 [==============================] - 7s 56ms/step - loss: 0.0122 - accuracy: 0.4774\n",
      "Epoch 596/1000\n",
      "121/121 [==============================] - 7s 57ms/step - loss: 0.0170 - accuracy: 0.4727\n",
      "Epoch 597/1000\n",
      "121/121 [==============================] - 7s 56ms/step - loss: 0.0132 - accuracy: 0.4715\n",
      "Epoch 598/1000\n",
      "121/121 [==============================] - 7s 57ms/step - loss: 0.0136 - accuracy: 0.4872\n",
      "Epoch 599/1000\n",
      "121/121 [==============================] - 7s 57ms/step - loss: 0.0205 - accuracy: 0.4747\n",
      "Epoch 600/1000\n",
      "121/121 [==============================] - 7s 56ms/step - loss: 0.0192 - accuracy: 0.4745\n",
      "Epoch 601/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0151 - accuracy: 0.4729\n",
      "Epoch 602/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0257 - accuracy: 0.4764\n",
      "Epoch 603/1000\n",
      "121/121 [==============================] - 7s 56ms/step - loss: 0.0233 - accuracy: 0.4717\n",
      "Epoch 604/1000\n",
      "121/121 [==============================] - 7s 56ms/step - loss: 0.0332 - accuracy: 0.4654\n",
      "Epoch 605/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0198 - accuracy: 0.4619\n",
      "Epoch 606/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0249 - accuracy: 0.4733\n",
      "Epoch 607/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0209 - accuracy: 0.4752\n",
      "Epoch 608/1000\n",
      "121/121 [==============================] - 7s 56ms/step - loss: 0.0149 - accuracy: 0.4810\n",
      "Epoch 609/1000\n",
      "121/121 [==============================] - 7s 56ms/step - loss: 0.0188 - accuracy: 0.4878\n",
      "Epoch 610/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0153 - accuracy: 0.4878\n",
      "Epoch 611/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0122 - accuracy: 0.4902\n",
      "Epoch 612/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0137 - accuracy: 0.4937\n",
      "Epoch 613/1000\n",
      "121/121 [==============================] - 7s 56ms/step - loss: 0.0179 - accuracy: 0.4864\n",
      "Epoch 614/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0222 - accuracy: 0.4922\n",
      "Epoch 615/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0173 - accuracy: 0.4808\n",
      "Epoch 616/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0133 - accuracy: 0.4769\n",
      "Epoch 617/1000\n",
      "121/121 [==============================] - 7s 56ms/step - loss: 0.0172 - accuracy: 0.4767\n",
      "Epoch 618/1000\n",
      "121/121 [==============================] - 7s 56ms/step - loss: 0.0147 - accuracy: 0.4862\n",
      "Epoch 619/1000\n",
      "121/121 [==============================] - 7s 56ms/step - loss: 0.0148 - accuracy: 0.4850\n",
      "Epoch 620/1000\n",
      "121/121 [==============================] - 7s 56ms/step - loss: 0.0214 - accuracy: 0.4800\n",
      "Epoch 621/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0364 - accuracy: 0.4814\n",
      "Epoch 622/1000\n",
      "121/121 [==============================] - 7s 57ms/step - loss: 0.0220 - accuracy: 0.4667\n",
      "Epoch 623/1000\n",
      "121/121 [==============================] - 7s 60ms/step - loss: 0.0209 - accuracy: 0.4789\n",
      "Epoch 624/1000\n",
      "121/121 [==============================] - 8s 63ms/step - loss: 0.0174 - accuracy: 0.4849\n",
      "Epoch 625/1000\n",
      "121/121 [==============================] - 7s 54ms/step - loss: 0.0210 - accuracy: 0.4989\n",
      "Epoch 626/1000\n",
      "121/121 [==============================] - 6s 53ms/step - loss: 0.0229 - accuracy: 0.4945\n",
      "Epoch 627/1000\n",
      "121/121 [==============================] - 6s 53ms/step - loss: 0.0163 - accuracy: 0.4872\n",
      "Epoch 628/1000\n",
      "121/121 [==============================] - 6s 53ms/step - loss: 0.0129 - accuracy: 0.4943\n",
      "Epoch 629/1000\n",
      "121/121 [==============================] - 6s 53ms/step - loss: 0.0156 - accuracy: 0.4979\n",
      "Epoch 630/1000\n",
      "121/121 [==============================] - 6s 53ms/step - loss: 0.0212 - accuracy: 0.4865\n",
      "Epoch 631/1000\n",
      "121/121 [==============================] - 241s 2s/step - loss: 0.0214 - accuracy: 0.4937\n",
      "Epoch 632/1000\n",
      "121/121 [==============================] - 7s 54ms/step - loss: 0.0258 - accuracy: 0.4816\n",
      "Epoch 633/1000\n",
      "121/121 [==============================] - 7s 55ms/step - loss: 0.0236 - accuracy: 0.4756\n",
      "Epoch 634/1000\n",
      "121/121 [==============================] - 7s 57ms/step - loss: 0.0113 - accuracy: 0.4853\n",
      "Epoch 635/1000\n",
      "121/121 [==============================] - 7s 60ms/step - loss: 0.0189 - accuracy: 0.4809\n",
      "Epoch 636/1000\n",
      "121/121 [==============================] - 7s 62ms/step - loss: 0.0119 - accuracy: 0.4791\n",
      "Epoch 637/1000\n",
      "121/121 [==============================] - 224s 2s/step - loss: 0.0131 - accuracy: 0.4798\n",
      "Epoch 638/1000\n",
      "121/121 [==============================] - 7s 56ms/step - loss: 0.0212 - accuracy: 0.4809\n",
      "Epoch 639/1000\n",
      "121/121 [==============================] - 6s 53ms/step - loss: 0.0120 - accuracy: 0.4931\n",
      "Epoch 640/1000\n",
      "121/121 [==============================] - 6s 53ms/step - loss: 0.0139 - accuracy: 0.4876\n",
      "Epoch 641/1000\n",
      "121/121 [==============================] - 6s 53ms/step - loss: 0.0131 - accuracy: 0.4837\n",
      "Epoch 642/1000\n",
      "121/121 [==============================] - 6s 54ms/step - loss: 0.0203 - accuracy: 0.4822\n",
      "Epoch 643/1000\n",
      "121/121 [==============================] - 7s 56ms/step - loss: 0.0174 - accuracy: 0.4846\n",
      "Epoch 644/1000\n",
      "121/121 [==============================] - 20s 167ms/step - loss: 0.0305 - accuracy: 0.4813\n",
      "Epoch 645/1000\n",
      "121/121 [==============================] - 8s 62ms/step - loss: 0.0174 - accuracy: 0.4904\n",
      "Epoch 646/1000\n",
      "121/121 [==============================] - 8s 68ms/step - loss: 0.0083 - accuracy: 0.4795\n",
      "Epoch 647/1000\n",
      " 88/121 [====================>.........] - ETA: 2s - loss: 0.0093 - accuracy: 0.4955"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m base_model\u001b[39m.\u001b[39;49mfit(final_arr_reshape, labels_arr_reshape, epochs \u001b[39m=\u001b[39;49m epochs,\n\u001b[1;32m      2\u001b[0m                batch_size \u001b[39m=\u001b[39;49m batch_size, \n\u001b[1;32m      3\u001b[0m                verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m) \n\u001b[1;32m      5\u001b[0m \u001b[39m# base_model.fit(base_model_data, base_model_labels, epochs = 100,\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39m#             #    batch_size = batch_size, \u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[39m#                verbose=1) \u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/DL/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/DL/lib/python3.10/site-packages/keras/engine/training.py:1650\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1642\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1643\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1644\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m   1648\u001b[0m ):\n\u001b[1;32m   1649\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1650\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1651\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1652\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/miniconda3/envs/DL/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/DL/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:880\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    877\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    879\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 880\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    882\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    883\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/miniconda3/envs/DL/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:912\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    909\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    910\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    911\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 912\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_no_variable_creation_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    913\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    914\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    915\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    916\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/miniconda3/envs/DL/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:134\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    132\u001b[0m   (concrete_function,\n\u001b[1;32m    133\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 134\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m    135\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m~/miniconda3/envs/DL/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1745\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1741\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1742\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1743\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1744\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1745\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   1746\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   1747\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m     args,\n\u001b[1;32m   1749\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1750\u001b[0m     executing_eagerly)\n\u001b[1;32m   1751\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/miniconda3/envs/DL/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:378\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    377\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 378\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    379\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    380\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    381\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    382\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    383\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    384\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    385\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    386\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    387\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    390\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    391\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/miniconda3/envs/DL/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "base_model.fit(final_arr_reshape, labels_arr_reshape, epochs = epochs,\n",
    "               batch_size = batch_size, \n",
    "               verbose=1) \n",
    "\n",
    "# base_model.fit(base_model_data, base_model_labels, epochs = 100,\n",
    "#             #    batch_size = batch_size, \n",
    "#                verbose=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5554, 0.6346510648727417)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loss, accuracy = base_model.evaluate(final_arr_reshape, labels_arr_reshape,\n",
    "#                                     #  batch_size = batch_size, \n",
    "#                                      verbose=0)\n",
    "\n",
    "loss, accuracy = base_model.evaluate(base_model_data, base_model_labels,\n",
    "                                    #  batch_size = batch_size, \n",
    "                                     verbose=0)\n",
    "\n",
    "round(loss,4), accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.save('../code/CNN_base_model.h5')\n",
    "base_model.save('../code/CNN_base_model.pkl')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "clf = MLPClassifier(solver='adam', shuffle=True, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sarah_prakriti_peters/miniconda3/envs/DL/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1098: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/sarah_prakriti_peters/miniconda3/envs/DL/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'activation': 'relu',\n",
       " 'alpha': 0.0001,\n",
       " 'batch_size': 'auto',\n",
       " 'beta_1': 0.9,\n",
       " 'beta_2': 0.999,\n",
       " 'early_stopping': False,\n",
       " 'epsilon': 1e-08,\n",
       " 'hidden_layer_sizes': (100,),\n",
       " 'learning_rate': 'constant',\n",
       " 'learning_rate_init': 0.001,\n",
       " 'max_fun': 15000,\n",
       " 'max_iter': 200,\n",
       " 'momentum': 0.9,\n",
       " 'n_iter_no_change': 10,\n",
       " 'nesterovs_momentum': True,\n",
       " 'power_t': 0.5,\n",
       " 'random_state': 1,\n",
       " 'shuffle': True,\n",
       " 'solver': 'adam',\n",
       " 'tol': 0.0001,\n",
       " 'validation_fraction': 0.1,\n",
       " 'verbose': False,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 417,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_arr_mlp = np.reshape(final_arr, (30735, 600))\n",
    "labels_arr_mlp = np.reshape(labels_arr, (30735,1))\n",
    "\n",
    "clf.fit(final_arr_mlp, labels_arr_mlp)\n",
    "clf.get_params()\n",
    "\n",
    "# print('Accuracy ', accuracy_score(y_test, clf.predict(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy  0.9113714006832602\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy ', accuracy_score(labels_arr_mlp, clf.predict(final_arr_mlp)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model \n",
    "#### LSTM Layer (128 units), Dropout layer (p=0.5), Dense layer (128 units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_29\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_6 (LSTM)               (30735, 128)              67584     \n",
      "                                                                 \n",
      " dropout_23 (Dropout)        (30735, 128)              0         \n",
      "                                                                 \n",
      " dense_66 (Dense)            (30735, 128)              16512     \n",
      "                                                                 \n",
      " dense_67 (Dense)            (30735, 1)                129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 84,225\n",
      "Trainable params: 84,225\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "base_model_data = np.reshape(final_arr, (30735, 200, 3)) \n",
    "base_model_labels = np.reshape(labels_arr, (30735,1))\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 50\n",
    "# prep_data = pd.read_csv('../data/good_again_bhas.csv')\n",
    "flatten = tf.keras.layers.Flatten()\n",
    "lstm_layer = tf.keras.layers.LSTM(units=128)\n",
    "dropout_layer = tf.keras.layers.Dropout(0.5)\n",
    "fc_layer = tf.keras.layers.Dense(units=128, activation='relu', kernel_regularizer = 'l2')\n",
    "fc_layer2 = tf.keras.layers.Dense(units=1, activation='softmax')\n",
    "lstm_model = tf.keras.Sequential([lstm_layer, dropout_layer,\n",
    "                                # flatten, \n",
    "                                fc_layer, fc_layer2])\n",
    "\n",
    "lstm_model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=False), \\\n",
    "                   optimizer=tf.keras.optimizers.Adam(0.03), \\\n",
    "                   metrics=[tf.keras.metrics.Accuracy()]) \n",
    "\n",
    "lstm_model.build((30735, 200, 3))\n",
    "lstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "961/961 [==============================] - 85s 86ms/step - loss: 0.6732 - accuracy: 0.6347\n",
      "Epoch 2/100\n",
      "961/961 [==============================] - 81s 85ms/step - loss: 0.6574 - accuracy: 0.6347\n",
      "Epoch 3/100\n",
      "961/961 [==============================] - 81s 84ms/step - loss: 0.6569 - accuracy: 0.6347\n",
      "Epoch 4/100\n",
      "961/961 [==============================] - 82s 85ms/step - loss: 0.6571 - accuracy: 0.6347\n",
      "Epoch 5/100\n",
      "961/961 [==============================] - 82s 85ms/step - loss: 0.6571 - accuracy: 0.6347\n",
      "Epoch 6/100\n",
      "961/961 [==============================] - 83s 86ms/step - loss: 0.6571 - accuracy: 0.6347\n",
      "Epoch 7/100\n",
      "266/961 [=======>......................] - ETA: 1:00 - loss: 0.6546 - accuracy: 0.6397"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[196], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m lstm_model\u001b[39m.\u001b[39;49mfit(base_model_data, base_model_labels, epochs \u001b[39m=\u001b[39;49m \u001b[39m100\u001b[39;49m,\n\u001b[1;32m      2\u001b[0m             \u001b[39m#    batch_size = batch_size, \u001b[39;49;00m\n\u001b[1;32m      3\u001b[0m                verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m) \n",
      "File \u001b[0;32m~/miniconda3/envs/DL/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/DL/lib/python3.10/site-packages/keras/engine/training.py:1650\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1642\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1643\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1644\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m   1648\u001b[0m ):\n\u001b[1;32m   1649\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1650\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1651\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1652\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/miniconda3/envs/DL/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/DL/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:880\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    877\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    879\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 880\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    882\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    883\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/miniconda3/envs/DL/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:912\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    909\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    910\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    911\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 912\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_no_variable_creation_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    913\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    914\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    915\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    916\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/miniconda3/envs/DL/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:134\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    132\u001b[0m   (concrete_function,\n\u001b[1;32m    133\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 134\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m    135\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m~/miniconda3/envs/DL/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1745\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1741\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1742\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1743\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1744\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1745\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   1746\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   1747\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m     args,\n\u001b[1;32m   1749\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1750\u001b[0m     executing_eagerly)\n\u001b[1;32m   1751\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/miniconda3/envs/DL/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:378\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    377\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 378\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    379\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    380\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    381\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    382\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    383\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    384\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    385\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    386\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    387\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    390\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    391\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/miniconda3/envs/DL/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bad pipe message: %s [b'\\xb9\\xe3\\xb2\\x16\\x96\\xe7\\xae\\xcd\\xd3\\x02\\x08M\\xd6\\xdc\\xa5u\\xeb\\xfd \\x11\\t\\xdc@ \\xa9>\\xb3\\xbb\\x88\\x18\\x94\\xca!B\\xab\\xbeaS\\xb6\\x88;\\x00\\xf5\\x15\\xc2\\x13A\\xd5M3@\\x00 ::\\x13\\x01\\x13\\x02\\x13\\x03\\xc0+\\xc0/\\xc0,\\xc00\\xcc\\xa9\\xcc\\xa8\\xc0\\x13\\xc0\\x14\\x00\\x9c\\x00\\x9d\\x00/\\x005\\x01\\x00\\x01\\x93\\xea\\xea\\x00\\x00\\x00\\x0b\\x00\\x02\\x01\\x00\\x00\\x17\\x00\\x00\\x00\\n\\x00\\n\\x00\\x08zz\\x00\\x1d\\x00\\x17\\x00\\x18\\x003\\x00+\\x00)zz\\x00\\x01\\x00\\x00\\x1d\\x00 lQ\\x8a\\x99\\x04\\xaf\\xc1\\x96\\x83\\x0f\\xa5,(\\xde1\\x18\\x9d\\xb1JEZ\"P\\xf5\\xba\\xf7\\xfdQy\\xe6{W\\x00\\x10\\x00\\x0e\\x00\\x0c\\x02h2\\x08http/1.1\\x00\\x12\\x00\\x00\\xff\\x01\\x00\\x01\\x00\\x00\\x05\\x00\\x05\\x01\\x00\\x00\\x00\\x00\\x00\\x1b\\x00\\x03\\x02\\x00\\x02\\x00-\\x00\\x02\\x01\\x01\\x00+\\x00\\x07\\x06jj\\x03\\x04\\x03\\x03\\x00#\\x00\\x00Di\\x00\\x05\\x00\\x03\\x02']\n",
      "Bad pipe message: %s [b'\\xfa}\\xe5\\xe9v\\r\\xc4a\\x8c\\xa0\\x9d\\x8d\\xb6a\\xb1$\\xbcG \\x12\\x9c2\\xab\\xb0\\xe0\\xbd3\\x00\\x8d\\xab\\xc5\\x86\\x81b.<\\xbb\\xaa\\xb6\\xd7\\xb6\\xc7\\x16+\\xb6f\\n\\x9d\\x91a\\xe4\\x00 zz\\x13\\x01\\x13\\x02\\x13\\x03\\xc0+\\xc0/\\xc0,\\xc00\\xcc\\xa9\\xcc\\xa8\\xc0\\x13\\xc0\\x14\\x00\\x9c\\x00\\x9d\\x00/\\x005\\x01\\x00\\x01\\x93\\xaa\\xaa\\x00\\x00\\x00\\x05\\x00\\x05\\x01\\x00\\x00\\x00\\x00\\x00#\\x00\\x00\\x00\\n\\x00\\n\\x00\\x08\\n\\n\\x00\\x1d\\x00\\x17\\x00', b'\\x01\\x00\\x01\\x00\\x003\\x00+\\x00)\\n\\n\\x00\\x01\\x00\\x00\\x1d\\x00 \\xb6\\xd4\\xbf\\xb5', b'\\xc1\\xe2Y\\xd6\\xfem\\xeam|f\\x93\\xe1:R\\x92\\xac\\x1a\\xc9\\x9b}jqw\\xf5\\xc2P\\x00+\\x00\\x07\\x06JJ\\x03\\x04\\x03\\x03\\x00-\\x00\\x02\\x01\\x01\\x00\\x0b\\x00\\x02\\x01\\x00\\x00\\x10\\x00\\x0e\\x00\\x0c\\x02h2\\x08http/1.1\\x00\\r\\x00\\x14\\x00\\x12\\x04\\x03\\x08\\x04\\x04\\x01\\x05\\x03\\x08\\x05\\x05\\x01\\x08\\x06\\x06\\x01\\x02\\x01\\x00\\x17\\x00\\x00\\x00\\x12\\x00\\x00Di\\x00\\x05\\x00\\x03\\x02h2\\x00\\x1b\\x00\\x03\\x02\\x00\\x02**\\x00\\x01\\x00\\x00\\x15\\x00\\xde']\n",
      "Bad pipe message: %s [b'\\x00\\r\\x00\\x12\\x00\\x10\\x04\\x03\\x08\\x04\\x04\\x01\\x05\\x03\\x08\\x05\\x05\\x01\\x08\\x06\\x06\\x01zz\\x00\\x01\\x00\\x00\\x15\\x00\\xe0\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00']\n"
     ]
    }
   ],
   "source": [
    "lstm_model.fit(base_model_data, base_model_labels, epochs = 100,\n",
    "            #    batch_size = batch_size, \n",
    "               verbose=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "961/961 [==============================] - 2s 1ms/step - loss: 0.5258 - accuracy: 0.6347\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5258, 0.6346510648727417)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bad pipe message: %s [b'\\x02P\\xcep\\xd4\\x9d\\xebA\\x8a7+\\x9a+\\xc0\\xbe\\x08\\xa8\\x9d 2\\xd4\\xb9\\xf5;\\xc4\\xe0\\xde\\xdeeKk\\x1e\\x01\\x8e\\xc3\\x86\\xb5\\x03\\x0e\\x8e\\x83\\x82@\\xfe\\x06\\x863Z\\x91V\\x87\\x00 **\\x13\\x01\\x13\\x02\\x13\\x03\\xc0+\\xc0/\\xc0,\\xc00\\xcc\\xa9\\xcc\\xa8\\xc0\\x13\\xc0\\x14\\x00\\x9c\\x00\\x9d\\x00/\\x005\\x01\\x00\\x01\\x93\\xba\\xba\\x00\\x00\\x00\\x05\\x00\\x05\\x01\\x00\\x00\\x00\\x00\\x00\\x1b\\x00\\x03\\x02\\x00\\x02\\x00\\x17\\x00\\x00\\x00\\n\\x00\\n\\x00\\x08\\x8a\\x8a\\x00\\x1d\\x00\\x17\\x00\\x18\\xff\\x01\\x00\\x01\\x00Di\\x00\\x05\\x00\\x03\\x02h2\\x00\\r\\x00\\x12\\x00\\x10\\x04\\x03\\x08\\x04\\x04\\x01\\x05\\x03\\x08\\x05\\x05\\x01\\x08\\x06\\x06\\x01\\x003\\x00+\\x00)\\x8a\\x8a\\x00\\x01\\x00\\x00\\x1d\\x00 \\x15\\xbb\\xcc+\\xdf\\xd6R\\xc2\\xf4i\\x96\\xd2{x^\\xe4#\\xd4>\\x9cq\\xb6[']\n",
      "Bad pipe message: %s [b'\\x8e\\x16\\xea\\xe5\\x02\\x89\\x08\\x00+\\x00\\x07\\x06\\xba\\xba\\x03\\x04\\x03\\x03\\x00\\x0b\\x00\\x02\\x01\\x00\\x00\\x10\\x00\\x0e\\x00\\x0c\\x02h2\\x08http/1.1\\x00-\\x00\\x02\\x01\\x01\\x00\\x12\\x00\\x00\\x00#\\x00\\x00\\xfa\\xfa\\x00\\x01\\x00\\x00\\x15\\x00\\xe0\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00']\n",
      "Bad pipe message: %s [b'R\\xdd\\xc8\\xca~u\\xa6>`\\x0fD;\\xfe\\x81\\x92\\x1ag( \\xf3\\xed\\xdf\\xda4\\x8d\\x08\\x17nE\\xbf\\x82w\\x9e\\x9b?z2\\x9d\\xe3\\x96\\xcc\\x1a(!\\xe3j\\x11U\\x95\\xd7\\x9b\\x00 jj\\x13\\x01\\x13\\x02\\x13\\x03\\xc0+\\xc0/\\xc0,\\xc00\\xcc\\xa9\\xcc\\xa8\\xc0\\x13\\xc0\\x14\\x00\\x9c\\x00\\x9d\\x00/\\x005\\x01\\x00\\x01\\x93\\xaa\\xaa\\x00\\x00\\xff\\x01\\x00\\x01\\x00\\x00\\x05\\x00\\x05\\x01\\x00\\x00\\x00\\x00Di\\x00\\x05\\x00\\x03\\x02h2\\x00-\\x00\\x02\\x01\\x01\\x00+\\x00\\x07\\x06\\xca\\xca\\x03\\x04\\x03\\x03\\x003\\x00+\\x00)jj\\x00\\x01\\x00\\x00\\x1d\\x00 \\xbf\\xec\\x06T\\x12\\xfa\\xd5\\x7f8z\\xdb\\x08D\"\\xfe\\xeaX\\xb6G\\x1e\\x1c\\xc5.\\xb8\\xee\\xe7m<&ova\\x00\\n\\x00\\n\\x00\\x08jj\\x00']\n",
      "Bad pipe message: %s [b'\\x17\\x00\\x18\\x00\\x10\\x00\\x0e\\x00\\x0c\\x02h2\\x08http/1.1\\x00\\x1b\\x00\\x03\\x02\\x00\\x02']\n",
      "Bad pipe message: %s [b'\\xcfdP<\\xe2\\xa0m\\x80\\xe7\\xe7~m\\xf6\\xdaI\\xe9\\xde\\xda \\x07I\\xaf\\x95\\x05\\xf8\\x1f\\xa3%', b'b2\\x9a\\x9ei\\xb0a\\x034\\xf4p\\x99\\xefmS\\\\\\x1eh\\x1e', b'\\x00 \\x1a\\x1a\\x13\\x01\\x13\\x02\\x13\\x03\\xc0+\\xc0/\\xc0,\\xc00\\xcc\\xa9\\xcc\\xa8\\xc0\\x13\\xc0\\x14\\x00\\x9c\\x00\\x9d\\x00/\\x005\\x01\\x00\\x01\\x93\\x8a\\x8a\\x00\\x00\\x00\\x0b\\x00\\x02\\x01\\x00\\x00\\x05\\x00\\x05\\x01\\x00\\x00\\x00\\x00\\x003\\x00+\\x00)\\xba\\xba\\x00\\x01\\x00\\x00\\x1d\\x00 \\x9dP\\xc3U\\tm\\xa0\\xc2\\x9b\\x9aS\\x01W6\\xbb\\x042\\xbb\\x17|\\xb4b\\x8d\\xefqh\\x14l<\\x15\\xb9\\x00\\x00\\x12\\x00\\x00\\x00\\n\\x00\\n\\x00\\x08\\xba\\xba\\x00\\x1d\\x00\\x17\\x00\\x18\\x00\\x1b\\x00\\x03\\x02\\x00\\x02\\x00-\\x00\\x02\\x01\\x01\\x00#\\x00\\x00']\n",
      "Bad pipe message: %s [b'r\\xbbx\\xae^U\\xf2o\\xee;\\xd7\\xcb\\xa5\\xce\\xcd\\x01\\x85\\xdf :\\x84<\\x0c\\xe0!//)\\x18\\xc0m\\xd9\\x14~\\xc3\\x14\\xe5\\xb0\\xd4[R\\xb8\\x17\\xf5\\xdb\\x88\\x98\\x88+;\\x17\\x00 \\x8a\\x8a\\x13\\x01\\x13\\x02\\x13\\x03\\xc0+\\xc0/\\xc0,\\xc00\\xcc\\xa9\\xcc\\xa8\\xc0\\x13\\xc0\\x14\\x00\\x9c\\x00\\x9d\\x00/\\x005\\x01\\x00\\x01\\x93zz\\x00\\x00\\x00#\\x00\\x00\\x00\\n\\x00\\n\\x00\\x08**\\x00\\x1d\\x00\\x17\\x00\\x18\\x00\\x05\\x00\\x05\\x01\\x00\\x00\\x00\\x00\\xff\\x01\\x00\\x01\\x00\\x00+\\x00\\x07\\x06\\xaa\\xaa\\x03\\x04\\x03\\x03']\n",
      "Bad pipe message: %s [b\"\\xf8N\\xac\\xf8\\xe9}\\x86'\\x9fd\\xb4\\xa6<\\xee\\xa7!\\xc2\\xaa \\xad\\xd1\\tV@\\xddu\\xd5R\\xb1\\x9e\\x19\\x19\\xac,\\xf6\\xca\\x16\\x8eOp\\xc7\\x83/\\xef\\x02\\x0c\\x17\\xcf0\\x1f\\xb9\\x00 \\xaa\\xaa\\x13\\x01\\x13\\x02\\x13\\x03\\xc0+\\xc0/\\xc0,\\xc00\\xcc\\xa9\\xcc\\xa8\\xc0\\x13\\xc0\\x14\\x00\\x9c\\x00\\x9d\\x00/\\x005\\x01\\x00\\x01\\x93ZZ\\x00\\x00\\x00\\x1b\\x00\\x03\\x02\\x00\\x02\\x00\\x12\\x00\\x00\\x00#\\x00\\x00\\x00\\x0b\\x00\\x02\\x01\\x00\\x00\\x10\\x00\\x0e\\x00\\x0c\\x02h2\\x08http\", b'.1\\x003\\x00+\\x00)JJ\\x00\\x01\\x00\\x00\\x1d\\x00 \\xc5\\x91\\xf5#Ls\\x8a\\xaasRW5\\xeeN\\xf6\\xadY35f\\xf2\\xce*\\x18\\xac2A\\x83\\x00', b'q\\x00\\n']\n",
      "Bad pipe message: %s [b\"o\\x8b'XF\\x82\\x02\\xae\\xc3\\x17\\xea\\xa6\\xb6\\x84\"]\n",
      "Bad pipe message: %s [b\"-\\x1e \\xbf\\x84\\xf1a\\xd0\\xd6\\xee\\x81#q\\x81*\\x89u\\xea\\xc5Pt\\x07I\\xd6\\x01\\xb0n\\xf3o'\\xf0h\\x18\\xfb\\x11\\x00 \\x1a\\x1a\\x13\\x01\\x13\\x02\\x13\\x03\\xc0+\\xc0/\\xc0,\\xc00\\xcc\\xa9\\xcc\\xa8\\xc0\\x13\\xc0\\x14\\x00\\x9c\\x00\\x9d\\x00/\\x005\\x01\\x00\\x01\\x93\\xba\\xba\\x00\\x00\\x00\\r\\x00\\x14\\x00\\x12\\x04\\x03\\x08\\x04\\x04\\x01\\x05\\x03\\x08\\x05\\x05\\x01\\x08\\x06\\x06\\x01\\x02\\x01\\x00\\n\\x00\\n\\x00\\x08\\xda\\xda\\x00\\x1d\\x00\\x17\\x00\\x18Di\\x00\", b'\\x03\\x02h2']\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = lstm_model.evaluate(base_model_data, base_model_labels,\n",
    "                                    #  batch_size = batch_size, \n",
    "                                     verbose=1)\n",
    "\n",
    "round(loss,4),accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/pickles/data.p\", 'rb') as data:\n",
    "    dd = pickle.load(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24087.309, 23677.01, 19497.0, 19497.0)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# np.array_equal(np.array(dd[\"X\"]), final_arr_reshape)\n",
    "# np.array(dd[\"X\"]).shape\n",
    "# final_arr_reshape.shape\n",
    "# np.sum(dd[\"X\"])\n",
    "# np.sum(final_arr_reshape)\n",
    "np.sum(dd[\"X\"]), np.sum(final_arr_reshape), np.sum(dd[\"Y\"]), np.sum(labels_arr_reshape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/pickles/data_slidingwindowFalse10_samplingrate20_df.p\", 'rb') as data1:\n",
    "    dd1 = pickle.load(data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>time</th>\n",
       "      <th>tac</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>614700.000000</td>\n",
       "      <td>614700.000000</td>\n",
       "      <td>614700.000000</td>\n",
       "      <td>6.147000e+05</td>\n",
       "      <td>614700.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.006241</td>\n",
       "      <td>0.007717</td>\n",
       "      <td>0.002854</td>\n",
       "      <td>1.493752e+09</td>\n",
       "      <td>0.634651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.138145</td>\n",
       "      <td>0.139681</td>\n",
       "      <td>0.128609</td>\n",
       "      <td>9.277910e+03</td>\n",
       "      <td>0.481528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-4.274800</td>\n",
       "      <td>-3.548100</td>\n",
       "      <td>-5.277200</td>\n",
       "      <td>1.493736e+09</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.009300</td>\n",
       "      <td>-0.006000</td>\n",
       "      <td>-0.007400</td>\n",
       "      <td>1.493744e+09</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.005600</td>\n",
       "      <td>1.493752e+09</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.008400</td>\n",
       "      <td>0.009500</td>\n",
       "      <td>0.011400</td>\n",
       "      <td>1.493760e+09</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>6.450300</td>\n",
       "      <td>5.344100</td>\n",
       "      <td>4.656500</td>\n",
       "      <td>1.493768e+09</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   x              y              z          time   \n",
       "count  614700.000000  614700.000000  614700.000000  6.147000e+05  \\\n",
       "mean       -0.006241       0.007717       0.002854  1.493752e+09   \n",
       "std         0.138145       0.139681       0.128609  9.277910e+03   \n",
       "min        -4.274800      -3.548100      -5.277200  1.493736e+09   \n",
       "25%        -0.009300      -0.006000      -0.007400  1.493744e+09   \n",
       "50%         0.000100       0.000100       0.005600  1.493752e+09   \n",
       "75%         0.008400       0.009500       0.011400  1.493760e+09   \n",
       "max         6.450300       5.344100       4.656500  1.493768e+09   \n",
       "\n",
       "                 tac  \n",
       "count  614700.000000  \n",
       "mean        0.634651  \n",
       "std         0.481528  \n",
       "min         0.000000  \n",
       "25%         0.000000  \n",
       "50%         1.000000  \n",
       "75%         1.000000  \n",
       "max         1.000000  "
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd1[\"data\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 ('csci1470')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d9ae1d7aab292b742cce88b46a7d39d3e8f6dede2c2111bf10ed84b03dba6379"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
